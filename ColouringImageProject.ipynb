{"cells":[{"cell_type":"markdown","source":["<h1>Image Recolouring Project</h1>\n","\n","Project developed by Alejandro Cano Caldero and Jesús Moncada Ramírez for the subject Neural Networks and Deep Learning, University of Padova, 2022-23.\n"],"metadata":{"id":"XRlCDiXymmvi"},"id":"XRlCDiXymmvi"},{"cell_type":"code","execution_count":7,"id":"33804e43","metadata":{"id":"33804e43","executionInfo":{"status":"ok","timestamp":1673707390422,"user_tz":-60,"elapsed":1,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"outputs":[],"source":["from PIL import Image\n","\n","import numpy as np\n","\n","import torch\n","\n","import matplotlib.pyplot as plt\n","\n","from torchvision import transforms, datasets\n","from torchvision.transforms import transforms\n","\n","from torch.utils.data import DataLoader, Dataset  \n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as op"]},{"cell_type":"code","execution_count":2,"id":"b6b86a5a","metadata":{"id":"b6b86a5a","executionInfo":{"status":"ok","timestamp":1673707171408,"user_tz":-60,"elapsed":348,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"outputs":[],"source":["# Define the execution device \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OyX2-8kfpi1z","outputId":"604eab80-8e08-4050-8dee-db855e3eb7f5","executionInfo":{"status":"ok","timestamp":1673707192642,"user_tz":-60,"elapsed":18364,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"OyX2-8kfpi1z","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["<h2>1. Dataset</h2>\n","\n","For the dataset we have used [ImageNette](https://github.com/fastai/imagenette), a reduced version of ImageNet, specifically the fill size images version."],"metadata":{"id":"yo4NF3Wam8QA"},"id":"yo4NF3Wam8QA"},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, image_path, transform=None):\n","        super(ImageDataset, self).__init__()\n","        self.data = datasets.ImageFolder(image_path,  transform)\n","\n","    def __getitem__(self, idx):\n","        x, y = self.data[idx]\n","        return x\n","\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"aGiWrxuRooTz","executionInfo":{"status":"ok","timestamp":1673707206504,"user_tz":-60,"elapsed":397,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"aGiWrxuRooTz","execution_count":4,"outputs":[]},{"cell_type":"code","source":["img_path = 'drive/MyDrive/imagenette2/train' # TODO: route in Google Drive\n","\n","# Define normalization [0, 255] --> [-1, 1] (Owing to the use of the tanh activation function)\n","\n","colored_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","grayscale_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Grayscale(3),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","colored_data = ImageDataset(img_path, colored_transform)\n","grayscale_data = ImageDataset(img_path, grayscale_transform)"],"metadata":{"id":"yIwpJ3fipDXP","executionInfo":{"status":"ok","timestamp":1673707222880,"user_tz":-60,"elapsed":10876,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"yIwpJ3fipDXP","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":11,"id":"83b897bd","metadata":{"id":"83b897bd","executionInfo":{"status":"ok","timestamp":1673707621719,"user_tz":-60,"elapsed":260,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"outputs":[],"source":["def print_image(img):\n","  plt.imshow(img.permute(1, 2, 0))"]},{"cell_type":"code","execution_count":null,"id":"e32d0b1e","metadata":{"id":"e32d0b1e"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels, filters, kernel_size, stride=1):\n","        super().__init__()\n","        \n","        self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=kernel_side, stride=2, padding=\"same\")\n","        self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=kernel_side, stride=2, padding=\"same\")\n","        self.layer2_bn = nn.BatchNorm2d(128)\n","        self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=kernel_side, stride=2, padding=\"same\")\n","        self.layer3_bn = nn.BatchNorm2d(256)\n","        self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=kernel_side, padding=\"same\") # stride = 1\n","        self.layer4_bn = nn.BatchNorm2d(512)\n","        self.layer5 = nn.Conv2d(512, out_channels=1, kernel_size=kernel_side, padding=\"same\")\n","    \n","    def forward(self, x):\n","        d = F.LeakyReLU(self.layer1(x), 0.2)\n","        d = F.LeakyReLU(self.layer2_bn(self.layer2(d)), 0.2)\n","        d = F.LeakyReLU(self.layer3_bn(self.layer3(d)), 0.2)\n","        d = F.LeakyReLU(self.layer4_bn(self.layer4(d)), 0.2)\n","        d = self.layer5(d)\n","        \n","        # Each (1×1) of the 30×30 represents a 70×70 dimension \n","        # in the input image (256×256), classifying a single patch of the original \n","        # image as real or fake.\n","        return F.Sigmoid(d)\n","        \n","\n","# model = Discriminator()\n","# model.to(device)\n","\n","# loss_fn = nn.BCELoss(weight=torch.tensor(0.5))\n","# optimizer = op.Adam(model.parameters(), lr=0.0002, weight_decay=0.5)"]},{"cell_type":"code","source":["class Generator(nn.Module):\n","  def __init__():\n","    pass\n","  def forward():\n","    pass"],"metadata":{"id":"g6SCuigGi7WC"},"id":"g6SCuigGi7WC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["adversarial_loss = nn.BCELoss(weight=torch.tensor(0.5))\n","l1_loss = nn.L1Loss()"],"metadata":{"id":"e45LZ-VXN-R5"},"id":"e45LZ-VXN-R5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generator_loss(generator_image, target_image, discriminator_predictions, real_target):\n","  gen_loss = adversarial_loss(discriminator_predictions, real_target)\n","  l1_l = l1_loss(generator_image, target_image)\n","  result = gen_loss + (100 * l1_l)\n","\n","  return result"],"metadata":{"id":"FhDadAG5OKD1"},"id":"FhDadAG5OKD1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def discriminator_loss(output, label):\n","  return adversarial_loss(output, label)"],"metadata":{"id":"lbRSIgnfOPDV"},"id":"lbRSIgnfOPDV","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}