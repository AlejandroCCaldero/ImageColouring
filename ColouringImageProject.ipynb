{"cells":[{"cell_type":"markdown","source":["#Image Recolouring Project\n","\n","Project developed by Alejandro Cano Caldero and Jesús Moncada Ramírez for the subject <i>Neural Networks and Deep Learning</i>, University of Padova, 2022-23.\n"],"metadata":{"id":"XRlCDiXymmvi"},"id":"XRlCDiXymmvi"},{"cell_type":"code","source":["!pip install lpips"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ripgenX5I17Y","executionInfo":{"status":"ok","timestamp":1674569398690,"user_tz":-60,"elapsed":8340,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}},"outputId":"906c5f4c-69d3-4f5f-f65f-d4cf28153d06"},"id":"ripgenX5I17Y","execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: lpips in /usr/local/lib/python3.8/dist-packages (0.1.4)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (0.14.1+cu116)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.13.1+cu116)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.7.3)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (4.64.1)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.0->lpips) (4.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.2.1->lpips) (2.25.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.2.1->lpips) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.24.3)\n"]}]},{"cell_type":"code","source":["!pip install torchmetrics[image]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zuVTRwfJEPr","executionInfo":{"status":"ok","timestamp":1674569403621,"user_tz":-60,"elapsed":4939,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}},"outputId":"3eef5480-0710-4848-97d1-377a05c5426c"},"id":"5zuVTRwfJEPr","execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics[image] in /usr/local/lib/python3.8/dist-packages (0.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (4.4.0)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (1.13.1+cu116)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (1.21.6)\n","Requirement already satisfied: torch-fidelity in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (0.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (1.7.3)\n","Requirement already satisfied: lpips in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (0.1.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from torchmetrics[image]) (0.14.1+cu116)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.8/dist-packages (from lpips->torchmetrics[image]) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->torchmetrics[image]) (2.25.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->torchmetrics[image]) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics[image]) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torchmetrics[image]) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torchmetrics[image]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torchmetrics[image]) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torchmetrics[image]) (2.10)\n"]}]},{"cell_type":"code","execution_count":70,"id":"33804e43","metadata":{"id":"33804e43","executionInfo":{"status":"ok","timestamp":1674569411661,"user_tz":-60,"elapsed":285,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"outputs":[],"source":["# Import necessary libraries for our project.\n","import numpy as np\n","\n","import torch\n","\n","import matplotlib.pyplot as plt\n","\n","from torchvision import transforms, datasets\n","from torchvision.transforms import transforms\n","\n","from torch.utils.data import DataLoader, Dataset  \n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as op\n","\n","from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n","\n","from torch import autograd"]},{"cell_type":"code","execution_count":71,"id":"b6b86a5a","metadata":{"id":"b6b86a5a","executionInfo":{"status":"ok","timestamp":1674569414940,"user_tz":-60,"elapsed":869,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"outputs":[],"source":["# Define the execution device.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# Mount in Google Drive. \n","# Only execute if running in Google Colab and the dataset is on Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OyX2-8kfpi1z","outputId":"11c6b381-7ca6-49b6-99f6-292e00865a18","executionInfo":{"status":"ok","timestamp":1674569420050,"user_tz":-60,"elapsed":2970,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"OyX2-8kfpi1z","execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["##1. Dataset\n","\n","The dataset we have used is [ImageNette](https://github.com/fastai/imagenette), a reduced version of ImageNet, the full size images version."],"metadata":{"id":"yo4NF3Wam8QA"},"id":"yo4NF3Wam8QA"},{"cell_type":"markdown","source":["<code>ImageDataset</code> is a default dataset class for images . To create an instance of this class you have to specify the route of the image folder (<code>image_path</code>) and a list of transforms (<code>transforms</code>). The constructor parameters <code>first</code> and <code>last</code> allow using a subset of the complete dataset specified in the path (as in our case). Anyway, the whole dataset is loaded, but only these elements will be available. \n","\n","Basically it is an adapted version of <code>torchvision.datasets.ImageFolder</code>. Note that only returns one image when accessing to an element in <code>\\_\\_getitem\\_\\_</code>."],"metadata":{"id":"RdFNSV_H7o5K"},"id":"RdFNSV_H7o5K"},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","  def __init__(self, image_path, transform=None, first=0, last=499):\n","    super(ImageDataset, self).__init__()\n","    self.data = datasets.ImageFolder(image_path,  transform)\n","    self.first = first\n","    self.last = last\n","\n","  def __getitem__(self, idx):\n","    x, y = self.data[self.first + idx]\n","    return x\n","\n","  def __len__(self):\n","    return self.last-self.first + 1"],"metadata":{"id":"aGiWrxuRooTz","executionInfo":{"status":"ok","timestamp":1674569425917,"user_tz":-60,"elapsed":274,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"aGiWrxuRooTz","execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["<code>TwoImagesDataset</code> is a dataset class combining two images datasets. We will use this class to create a dataset with tuples containing two versions of the same image, the first element will be the grayscale version (input image) and the second element will be the colored version (target image)."],"metadata":{"id":"W_eYsVQa8caz"},"id":"W_eYsVQa8caz"},{"cell_type":"code","source":["class TwoImagesDataset(Dataset):\n","  def __init__(self, dataset1, dataset2):\n","    super(TwoImagesDataset, self).__init__()\n","    self.dataset1 = dataset1\n","    self.dataset2 = dataset2\n","    if len(self.dataset1) != len(self.dataset2):\n","      print('ERROR, len of dataset1 is different from len of dataset2')\n","\n","  def __getitem__(self, idx):\n","    return self.dataset1[idx], self.dataset2[idx]\n","\n","  def __len__(self):\n","    return len(self.dataset1)"],"metadata":{"id":"UEXGx53VUtuq","executionInfo":{"status":"ok","timestamp":1674569431053,"user_tz":-60,"elapsed":476,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"UEXGx53VUtuq","execution_count":74,"outputs":[]},{"cell_type":"markdown","source":["After defining the data structures, we will create a dataset class with all the images resized to <code>256 x 256</code> (<code>colored_data</code>) and a dataset class with all the images resized to <code>256 x 256</code> in grayscale (<code>grayscale_data</code>). By last we will create the dataset class containing both kind of images (<code>dataset</code>), that will be used for training. In our case, we have decided it to have 500 samples.\n","\n","We will also create a test dataset with 150 samples (30% of the training dataset), <code>dataset_test</code>."],"metadata":{"id":"bixeLxG4-3A4"},"id":"bixeLxG4-3A4"},{"cell_type":"code","source":["# If locating the dataset in Google Drive\n","img_path_train = 'drive/MyDrive/imagenette2/train'\n","img_path_test = 'drive/MyDrive/imagenette2/val'\n","\n","# Define normalization [0, 255] --> [-1, 1] (Owing to the use of the tanh activation function)\n","\n","colored_transform = transforms.Compose(\n","    [transforms.ToTensor(), # to tensor\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # normalization [0, 255] --> [-1, 1] (to the tanh activation function)\n","     transforms.Resize((256, 256)) # resize to (256, 256)\n","     ] )\n","\n","grayscale_transform = transforms.Compose(\n","    [transforms.ToTensor(), # to tensor\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # normalization [0, 255] --> [-1, 1] (to the tanh activation function)\n","     transforms.Resize((256, 256)), # resize to (256, 256)\n","     transforms.Grayscale(3) # to grayscale\n","     ])\n","\n","# TRAINING SET 500 samples\n","colored_data = ImageDataset(img_path_train, colored_transform, first=0, last=499)\n","grayscale_data = ImageDataset(img_path_train, grayscale_transform, first=0, last=499)\n","dataset = TwoImagesDataset(grayscale_data, colored_data)\n","\n","# TEST SET 150 samples (30% of training samples)\n","colored_data_test = ImageDataset(img_path_test, colored_transform, first=0, last=149)\n","grayscale_data_test = ImageDataset(img_path_test, grayscale_transform, first=0, last=149)\n","dataset_test = TwoImagesDataset(colored_data_test, grayscale_data_test)"],"metadata":{"id":"yIwpJ3fipDXP","executionInfo":{"status":"ok","timestamp":1674569481259,"user_tz":-60,"elapsed":1117,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"yIwpJ3fipDXP","execution_count":75,"outputs":[]},{"cell_type":"code","source":["print(\"Number of images in our training dataset is: {0}\".format(len(dataset)))\n","print(\"Number of images in our test dataset is: {0}\".format(len(dataset_test)))"],"metadata":{"id":"y_EFSLiPUob9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674569488636,"user_tz":-60,"elapsed":290,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}},"outputId":"3f72e5e4-2055-407d-eaf0-5e5c0ae1c3d0"},"id":"y_EFSLiPUob9","execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in our training dataset is: 10\n","Number of images in our test dataset is: 10\n"]}]},{"cell_type":"markdown","source":["To check everything was ok let's show (with a new function <code>plot_image</code>) the colored and the grayscale version of the same image."],"metadata":{"id":"oOYuE1Cv_ZtA"},"id":"oOYuE1Cv_ZtA"},{"cell_type":"code","execution_count":null,"id":"83b897bd","metadata":{"id":"83b897bd"},"outputs":[],"source":["def plot_image(img):\n","  plt.imshow(img.permute(1, 2, 0))\n","\n","grayscale_img, colored_img  = dataset[0]\n","\n","print('Plotting a grayscale image in the dataset:')\n","plot_image(grayscale_img)"]},{"cell_type":"code","source":["print('Plotting a colored image in the dataset:')\n","plot_image(colored_img)"],"metadata":{"id":"w2KWTWqXBlkX"},"id":"w2KWTWqXBlkX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will also create a function to show images where each element takes value from -1 to 1, <code>plot_image_minus1to1</code>."],"metadata":{"id":"ZagQbS7imaEx"},"id":"ZagQbS7imaEx"},{"cell_type":"code","source":["def plot_image_minus1to1(image):\n","  plot_image((image + 1) / 2)"],"metadata":{"id":"jYPsZBBlK4lA","executionInfo":{"status":"ok","timestamp":1674569526856,"user_tz":-60,"elapsed":3,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"jYPsZBBlK4lA","execution_count":81,"outputs":[]},{"cell_type":"markdown","source":["We need a <code>DataLoader</code> instance for our dataset in order to get the images as batchs for the training phase. The <code>batch_size</code> will be 1 as said in the paper. We will also create a dataloader for the test data."],"metadata":{"id":"RUEuLCNQnJxK"},"id":"RUEuLCNQnJxK"},{"cell_type":"code","source":["dataloader = DataLoader(dataset, batch_size=1)\n","\n","dataloader_test = DataLoader(dataset_test, batch_size=1)"],"metadata":{"id":"55cLAHs-Mlpe","executionInfo":{"status":"ok","timestamp":1674569568635,"user_tz":-60,"elapsed":299,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"55cLAHs-Mlpe","execution_count":83,"outputs":[]},{"cell_type":"markdown","source":["##2. Discriminator"],"metadata":{"id":"b95tH6-FB7nE"},"id":"b95tH6-FB7nE"},{"cell_type":"markdown","source":["The discriminator used (called PatchGAN) takes as input an image of <code>(256x256)</code> and returns a tensor of <code>(30x30)</code>. The discriminator class is the following:"],"metadata":{"id":"6NSU-t1rCKlD"},"id":"6NSU-t1rCKlD"},{"cell_type":"code","execution_count":84,"id":"e32d0b1e","metadata":{"id":"e32d0b1e","executionInfo":{"status":"ok","timestamp":1674569571431,"user_tz":-60,"elapsed":786,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels, kernel_size, stride=1):\n","        super().__init__()\n","        \n","        self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2_bn = nn.BatchNorm2d(128)\n","        self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer3_bn = nn.BatchNorm2d(256)\n","        self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=kernel_size, padding=1) # stride = 1\n","        self.layer4_bn = nn.BatchNorm2d(512)\n","        self.layer5 = nn.Conv2d(512, out_channels=1, kernel_size=kernel_size, padding=1)\n","        \n","        # Weight initialization\n","        torch.nn.init.normal_(self.layer1.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer2.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer3.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer4.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer5.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer2_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer2_bn.bias.data, 0.0)\n","        torch.nn.init.normal_(self.layer3_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer3_bn.bias.data, 0.0)\n","        torch.nn.init.normal_(self.layer4_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer4_bn.bias.data, 0.0)\n","\n","    def forward(self, x, verbose=False):\n","        d = F.leaky_relu(self.layer1(x), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer2_bn(self.layer2(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer3_bn(self.layer3(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer4_bn(self.layer4(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = self.layer5(d)\n","        if verbose:\n","          print(d.shape)\n","\n","        return torch.sigmoid(d)"]},{"cell_type":"markdown","source":["Let's create an object for the discriminator, and its Adam optimizer."],"metadata":{"id":"8n-zLnNECNXw"},"id":"8n-zLnNECNXw"},{"cell_type":"code","source":["discriminator = Discriminator(in_channels=6, kernel_size=4)\n","discriminator.to(device)\n","\n","discriminator_optimizer = op.Adam(discriminator.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"2jZyxqWrCQCp","executionInfo":{"status":"ok","timestamp":1674569575831,"user_tz":-60,"elapsed":573,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"2jZyxqWrCQCp","execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["And let's execute the discriminator forward pass on a pair of images concatenated from the dataset to check if it works."],"metadata":{"id":"SEyuFfkFCXSt"},"id":"SEyuFfkFCXSt"},{"cell_type":"code","source":["input_img, target_img = dataset[0]\n","\n","input_img = input_img.unsqueeze(0)\n","target_img = target_img.unsqueeze(0)\n","# unsqueeze(0) needed to get batch_size = 1\n","\n","disc_input = torch.cat((input_img, target_img), dim=1) # discriminator input\n","\n","print(\"Shape of discriminator input: {0}\".format(disc_input.shape))\n","\n","out = discriminator.forward(disc_input) # forward pass discriminator\n","\n","print(\"Shape of discriminator output: {0}\".format(out.shape))"],"metadata":{"id":"VQUWlIcaQC5W"},"id":"VQUWlIcaQC5W","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3. Generator"],"metadata":{"id":"CEAnLosJ7B2Y"},"id":"CEAnLosJ7B2Y"},{"cell_type":"markdown","source":["The generator used has an autoencoder style, with both encoder and decoder networks. The input and the output are images. It uses a UNET Generator, with skip connections.The generator class is the following:"],"metadata":{"id":"OI8Jt-zg7Ga5"},"id":"OI8Jt-zg7Ga5"},{"cell_type":"code","source":["class Generator(nn.Module):\n","  def __init__(self, in_channels, stride=1):\n","    super().__init__()\n","\n","    # ----------------------------- ENCODER ----------------------------\n","    #            ENCODER MODEL: C64-C128-C256-C512-C512-C512-C512\n","    self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=4, stride=2, padding=1)\n","    self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=4, stride=2, padding=1)\n","    self.layer2_bn = nn.BatchNorm2d(128)\n","    self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=4, stride=2, padding=1)\n","    self.layer3_bn = nn.BatchNorm2d(256)\n","    self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer4_bn = nn.BatchNorm2d(512)\n","    self.layer5 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer5_bn = nn.BatchNorm2d(512)\n","    self.layer6 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer6_bn = nn.BatchNorm2d(512)\n","    self.layer7 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer7_bn = nn.BatchNorm2d(512)\n","\n","    # ----------------------------- BOTTLENECK ----------------------------\n","    self.bottleneck_layer = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","\n","    # ----------------------------- DECODER ----------------------------\n","    #           DECODER MODEL: CD512-CD512-CD512-CD512-CD256-CD128-CD64\n","    self.layer8 = nn.ConvTranspose2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer8_bn = nn.BatchNorm2d(512)\n","    self.layer8_dpout = nn.Dropout()\n","    self.layer9 = nn.ConvTranspose2d(1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer9_bn = nn.BatchNorm2d(512)\n","    self.layer9_dpout = nn.Dropout()\n","    self.layer10 = nn.ConvTranspose2d(1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer10_bn = nn.BatchNorm2d(512)\n","    self.layer10_dpout = nn.Dropout()\n","    self.layer11 = nn.ConvTranspose2d(1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer11_bn = nn.BatchNorm2d(512)\n","    self.layer12 = nn.ConvTranspose2d(1024, out_channels=256, kernel_size=4, stride=2, padding=1)\n","    self.layer12_bn = nn.BatchNorm2d(256)\n","    self.layer13 = nn.ConvTranspose2d(512, out_channels=128, kernel_size=4, stride=2, padding=1)\n","    self.layer13_bn = nn.BatchNorm2d(128)\n","    self.layer14 = nn.ConvTranspose2d(256, out_channels=64, kernel_size=4, stride=2, padding=1)\n","    self.layer14_bn = nn.BatchNorm2d(64)\n","\n","    # ----------------------------- OUTPUT ----------------------------\n","    self.layer15 = nn.ConvTranspose2d(128, out_channels=3, kernel_size=4, stride=2, padding=1)\n","\n","    # Weight initialization\n","    torch.nn.init.normal_(self.layer1.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer2.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer3.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer4.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer5.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer6.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer7.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.bottleneck_layer.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer8.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer9.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer10.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer11.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer12.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer13.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer14.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer15.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer2_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer2_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer3_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer3_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer4_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer4_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer5_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer5_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer6_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer6_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer7_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer7_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer8_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer8_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer9_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer9_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer10_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer10_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer11_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer11_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer12_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer12_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer13_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer13_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer14_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer14_bn.bias.data, 0.0)\n","\n","  def forward(self, x, verbose=False):\n","\n","    # ----------------------------- ENCODER ----------------------------\n","    e1 = F.leaky_relu(self.layer1(x), 0.2)\n","    if verbose:\n","      print(e1.shape)\n","    e2 = F.leaky_relu(self.layer2_bn(self.layer2(e1)), 0.2)\n","    if verbose:\n","      print(e2.shape)\n","    e3 = F.leaky_relu(self.layer3_bn(self.layer3(e2)), 0.2)\n","    if verbose:\n","      print(e3.shape)\n","    e4 = F.leaky_relu(self.layer4_bn(self.layer4(e3)), 0.2)\n","    if verbose:\n","      print(e4.shape)\n","    e5 = F.leaky_relu(self.layer5_bn(self.layer5(e4)), 0.2)\n","    if verbose:\n","      print(e5.shape)\n","    e6 = F.leaky_relu(self.layer6_bn(self.layer6(e5)), 0.2)\n","    if verbose:\n","      print(e6.shape)\n","    e7 = F.leaky_relu(self.layer7_bn(self.layer7(e6)), 0.2) \n","    if verbose:\n","      print(e7.shape)\n","\n","    # ----------------------------- BOTTLENECK ----------------------------\n","    b = F.relu(self.bottleneck_layer(e7))\n","    if verbose:\n","      print(b.shape)\n","\n","    # ----------------------------- DECODER ----------------------------\n","    d1 = F.relu(torch.cat((self.layer8_dpout(self.layer8_bn(self.layer8(b))), e7), 1))\n","    if verbose:\n","      print(d1.shape)\n","    d2 = F.relu(torch.cat((self.layer9_dpout(self.layer9_bn(self.layer9(d1))), e6), 1))\n","    if verbose:\n","      print(d2.shape)\n","    d3 = F.relu(torch.cat((self.layer10_dpout(self.layer10_bn(self.layer10(d2))), e5), 1))\n","    if verbose:\n","      print(d3.shape)\n","    d4 = F.relu(torch.cat(((self.layer11_bn(self.layer11(d3))), e4), 1))\n","    if verbose:\n","      print(d4.shape)\n","    d5 = F.relu(torch.cat(((self.layer12_bn(self.layer12(d4))), e3), 1))\n","    if verbose:\n","      print(d5.shape)\n","    d6 = F.relu(torch.cat(((self.layer13_bn(self.layer13(d5))), e2), 1))\n","    if verbose:\n","      print(d6.shape)\n","    d7 = F.relu(torch.cat(((self.layer14_bn(self.layer14(d6))), e1), 1))\n","    if verbose:\n","      print(d7.shape)\n","\n","    # ----------------------------- OUTPUT ----------------------------\n","    o = torch.tanh(self.layer15(d7))\n","\n","    return o"],"metadata":{"id":"g6SCuigGi7WC","executionInfo":{"status":"ok","timestamp":1674569584884,"user_tz":-60,"elapsed":275,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"g6SCuigGi7WC","execution_count":87,"outputs":[]},{"cell_type":"markdown","source":["Let's create an object for the generator, and its Adam optimizer."],"metadata":{"id":"3F_lufO5s6_o"},"id":"3F_lufO5s6_o"},{"cell_type":"code","source":["generator = Generator(3)\n","generator.to(device)\n","\n","generator_optimizer = op.Adam(generator.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"w90yleohsi_C","executionInfo":{"status":"ok","timestamp":1674569595512,"user_tz":-60,"elapsed":1408,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"w90yleohsi_C","execution_count":88,"outputs":[]},{"cell_type":"markdown","source":["And let's execute the generator forward pass on an images from the dataset to check if it works."],"metadata":{"id":"3J25gYESxm9z"},"id":"3J25gYESxm9z"},{"cell_type":"code","source":["input_img, tg_img = dataset[0]\n","input = input_img.unsqueeze(0)\n","\n","print(\"Shape of generator input: {0}\".format(input.shape))\n","\n","out = generator.forward(input)\n","\n","print(\"Shape of generator output: {0} \".format(out.shape))\n","\n","out = out.detach()\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"OdfwyxIqszxO"},"id":"OdfwyxIqszxO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##4. Loss functions"],"metadata":{"id":"A5BzSbPh_ahG"},"id":"A5BzSbPh_ahG"},{"cell_type":"markdown","source":["For the loss function, we are going to use the **Binary Cross-Entropy** loss and the **L1** loss."],"metadata":{"id":"UFgM1oGG-3gC"},"id":"UFgM1oGG-3gC"},{"cell_type":"code","source":["adversarial_loss = nn.BCELoss(weight=torch.tensor(0.5))\n","l1_loss = nn.L1Loss()"],"metadata":{"id":"e45LZ-VXN-R5","executionInfo":{"status":"ok","timestamp":1674569605005,"user_tz":-60,"elapsed":269,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"e45LZ-VXN-R5","execution_count":90,"outputs":[]},{"cell_type":"markdown","source":["Four parameters are fed to the <code>generator_loss</code> function:\n","\n","1. <code>generator_image</code>: Images produced by the generator\n","2. <code>target_image</code>: Ground-truth pair image for the input fed to the generator.\n","3. <code>discriminator_predictions</code>: Output predictions from the discriminator, when fed with generator-produced images.\n","4. <code>real_target</code>: Ground-truth labels (1), as you would like the generator to produce real images by fooling the discriminator. The labels therefore would be one."],"metadata":{"id":"IjfuXc569pNv"},"id":"IjfuXc569pNv"},{"cell_type":"code","source":["def generator_loss(generator_image, target_image, discriminator_predictions, real_target):\n","  gen_loss = adversarial_loss(discriminator_predictions, real_target)\n","  l1_l = l1_loss(generator_image, target_image)\n","  result = gen_loss + (100 * l1_l)\n","\n","  return result"],"metadata":{"id":"FhDadAG5OKD1","executionInfo":{"status":"ok","timestamp":1674569619389,"user_tz":-60,"elapsed":405,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"FhDadAG5OKD1","execution_count":91,"outputs":[]},{"cell_type":"markdown","source":["The <code>discriminator_loss</code> function has 2 arguments:\n","\n","1. <code>output</code>: Output of the discriminator for a pair of images as input.\n","2. <code>label</code>: Expected output for the discriminator, a tensor full of 0's (in case of fake images) or a tensor full of 1's (in case of true images)."],"metadata":{"id":"4kCeF_S5-nvG"},"id":"4kCeF_S5-nvG"},{"cell_type":"code","source":["def discriminator_loss(output, label):\n","  return adversarial_loss(output, label)"],"metadata":{"id":"lbRSIgnfOPDV","executionInfo":{"status":"ok","timestamp":1674569622048,"user_tz":-60,"elapsed":835,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"lbRSIgnfOPDV","execution_count":92,"outputs":[]},{"cell_type":"markdown","source":["##5. Training"],"metadata":{"id":"iQlLrv5DM5ug"},"id":"iQlLrv5DM5ug"},{"cell_type":"markdown","source":["First of all let's create some functions to save and load the models (generators and discriminator) trained."],"metadata":{"id":"V0_hBtbCJgKP"},"id":"V0_hBtbCJgKP"},{"cell_type":"code","source":["def save_model(model, path, verbose=False):\n","  torch.save(model.state_dict(), path)\n","  if verbose:\n","    print('Saved model '+path)\n","\n","def load_model(model, path, verbose=False):\n","  # model must match with the saved data in path\n","  if verbose:\n","    print('Loading model '+path)\n","  return model.load_state_dict(torch.load(path))"],"metadata":{"id":"55zRjWTjJmXS","executionInfo":{"status":"ok","timestamp":1674570012849,"user_tz":-60,"elapsed":291,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"55zRjWTjJmXS","execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["The training phase for our model will be the following. Note <code>num_epochs</code> has been set to 1 in order to test everything, but should be a higher value. The training losses (from generator and discriminator) obtained at each batch processed will be saved into two list, in order to plot them later."],"metadata":{"id":"mh3GLO7ZvMWg"},"id":"mh3GLO7ZvMWg"},{"cell_type":"code","source":["# Info to save the models\n","generator_model_path = \"drive/MyDrive/models/generatorModelImageColoring.pth\"\n","discriminator_model_path = \"drive/MyDrive/models/discriminatorModelImageColoring.pth\"\n","\n","# Number of epochs\n","num_epochs = 10 \n","\n","# Count\n","cnt = 1\n","\n","# List of generator losses\n","generator_losses = list()\n","# List of discriminator losses\n","discriminator_losses = list()\n","\n","for epoch in range(1, num_epochs + 1):\n","\n","  for (input_img, tg_img) in dataloader:\n","\n","    discriminator_optimizer.zero_grad()\n","\n","    # Put input and target image into device.\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    # Ground truth labels real and fake\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    # Generator forward pass\n","    generated_image = generator(input_img)\n","\n","    # Train discriminator with fake/generated images\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","\n","    discriminator_fake = discriminator(disc_inp_fake.detach())\n","    discriminator_fake_loss = discriminator_loss(discriminator_fake, fake_target)\n","\n","    # Train discriminator with real images\n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","\n","    discriminator_real = discriminator(disc_inp_real)\n","    discriminator_real_loss = discriminator_loss(discriminator_real,  real_target)\n","\n","    # Average discriminator loss\n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    # Save discriminator loss into the list\n","    discriminator_losses.append(discriminator_total_loss.item())\n","\n","    # Compute gradients and run optimizer step\n","    discriminator_total_loss.backward()\n","    discriminator_optimizer.step()\n","\n","    # Train generator with real labels\n","    generator_optimizer.zero_grad()\n","\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = discriminator(fake_gen)\n","\n","    gen_loss = generator_loss(generated_image, tg_img, discriminator_prediction, real_target)                              \n","\n","    # Save generator loss into the list\n","    generator_losses.append(gen_loss.item())\n","\n","    # Compute gradients and run optimizer step\n","    gen_loss.backward()\n","    generator_optimizer.step()\n","\n","    # Notify state of training and save models\n","    if cnt % 10 == 0:\n","      print(\"Number of processed batches (batch_size = 1): \", cnt)\n","      save_model(generator, generator_model_path)\n","      save_model(discriminator, discriminator_model_path)\n","\n","    cnt = cnt + 1"],"metadata":{"id":"tq6PHXr0L9zx"},"id":"tq6PHXr0L9zx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have coded two functions to plot the generator loss (<code>plot_gen_loss</code>) and the discriminator loss (<code>plot_gen_loss</code>) over the training."],"metadata":{"id":"TXFTsrwZIlJN"},"id":"TXFTsrwZIlJN"},{"cell_type":"code","source":["def plot_disc_loss(d_losses):\n","\n","  fig = plt.figure()\n","  ax = plt.axes()\n","\n","  plt.title(\"Discriminator loss\")\n","  plt.xlabel(\"Number of batches processed\")\n","  plt.ylabel(\"Discriminator loss\");\n","\n","  x = list(range(1, len(d_losses)+1))\n","\n","  ax.plot(x, d_losses)\n","\n","def plot_gen_loss(g_losses):\n","\n","  fig = plt.figure()\n","  ax = plt.axes()\n","\n","  plt.title(\"Generator loss\")\n","  plt.xlabel(\"Number of batches processed\")\n","  plt.ylabel(\"Generator loss\");\n","\n","  x = list(range(1, len(g_losses)+1))\n","\n","  ax.plot(x, g_losses)"],"metadata":{"id":"ea9jw7XbmRPr","executionInfo":{"status":"ok","timestamp":1674570069434,"user_tz":-60,"elapsed":329,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"ea9jw7XbmRPr","execution_count":100,"outputs":[]},{"cell_type":"markdown","source":["Let's see the plots."],"metadata":{"id":"-_xbXLKkI_pR"},"id":"-_xbXLKkI_pR"},{"cell_type":"code","source":["plot_disc_loss(discriminator_losses)\n","plot_gen_loss(generator_losses)"],"metadata":{"id":"qKUnY5zsxfJa"},"id":"qKUnY5zsxfJa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's test the model we obtained showing the generator output for one input image.\n"],"metadata":{"id":"gRRpIfCawidf"},"id":"gRRpIfCawidf"},{"cell_type":"markdown","source":["Let's try to save and load the models form disc. Anyway, they have been saved each 10 iterations during training."],"metadata":{"id":"wEo6xMNuARDu"},"id":"wEo6xMNuARDu"},{"cell_type":"code","source":["save_model(generator, generator_model_path, verbose=True)\n","save_model(discriminator, discriminator_model_path, verbose=True)"],"metadata":{"id":"BQDjnYFq5fpu"},"id":"BQDjnYFq5fpu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = Generator(3)\n","load_model(generator, generator_model_path)\n","\n","discriminator = Discriminator(in_channels=6, kernel_size=4)\n","load_model(discriminator, discriminator_model_path, verbose=True)"],"metadata":{"id":"gi6POkY4AovX"},"id":"gi6POkY4AovX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Test"],"metadata":{"id":"bARc6yNZuWqf"},"id":"bARc6yNZuWqf"},{"cell_type":"markdown","source":["We will execute our generator on the first image of the training dataset, to see if it's working properly."],"metadata":{"id":"aMY9iTYoKndD"},"id":"aMY9iTYoKndD"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"3PH9qqYy4ok_"},"id":"3PH9qqYy4ok_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Input image:')\n","plot_image(input_img[0])"],"metadata":{"id":"Ay3Zc6uB5BWZ"},"id":"Ay3Zc6uB5BWZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["No we implement a function to calculate the test error of our model, its arguments are:\n","- <code>data</code>: <code>DataLoader</code> object with the test dataset.\n","- <code>g</code>: generator instance.\n","- <code>d</code>: discriminator instance.\n","- <code>g_l</code>: generator loss function.\n","- <code>d_l</code>: discriminator loss function\n","It will return the generator test error and the discriminator test error in a tuple."],"metadata":{"id":"isJqpedNKyvR"},"id":"isJqpedNKyvR"},{"cell_type":"code","source":["def get_test_error(data, g, d, g_l, d_l):\n","  gen_loss_return = 0.0\n","  dis_loss_return = 0.0\n","  \n","  for (input_img, tg_img) in data:\n","\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = g(input_img)\n","\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","\n","    discriminator_fake = d(disc_inp_fake.detach())\n","    discriminator_fake_loss = d_l(discriminator_fake, fake_target)\n","\n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","\n","    discriminator_real = d(disc_inp_real)\n","    discriminator_real_loss = d_l(discriminator_real,  real_target)\n","\n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    dis_loss_return = dis_loss_return + discriminator_total_loss.item()\n","\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    \n","    discriminator_prediction = d(fake_gen)\n","    gen_loss = g_l(generated_image, tg_img, discriminator_prediction, real_target).item()                            \n","\n","    gen_loss_return = gen_loss_return + gen_loss\n","  \n","  return gen_loss_return / len(data), dis_loss_return / len(data)"],"metadata":{"id":"e7BKHuSTuX-w","executionInfo":{"status":"ok","timestamp":1674570446864,"user_tz":-60,"elapsed":270,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"e7BKHuSTuX-w","execution_count":107,"outputs":[]},{"cell_type":"markdown","source":["So the test error for our model are:"],"metadata":{"id":"4CQxiU2iLVex"},"id":"4CQxiU2iLVex"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator, discriminator, generator_loss, discriminator_loss))"],"metadata":{"id":"VDI-n7KHvciW"},"id":"VDI-n7KHvciW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Using additional perceptual losses (<code>torchmetrics</code>)\n"],"metadata":{"id":"NNv8F8e1qQq6"},"id":"NNv8F8e1qQq6"},{"cell_type":"markdown","source":["### Changes introduced"],"metadata":{"id":"Yky3JHPEy3ti"},"id":"Yky3JHPEy3ti"},{"cell_type":"markdown","source":["The first improvement we have implemented, <code>LearnedPerceptualImagePatchSimilarity</code>, is used to judge the perceptual similarity between two images. This loss will be added to the generator loss, to penalize those generated images that are not similar to their input images."],"metadata":{"id":"fnN-0Gy_rB7e"},"id":"fnN-0Gy_rB7e"},{"cell_type":"code","source":["lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex')"],"metadata":{"id":"a4LRnxp4qZ_a"},"id":"a4LRnxp4qZ_a","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This way, we have to define a new generator loss adding the perceptual loss. We have decided to multiply its value by 50 to make it more important than the _adversarial_ loss but less than the _l1_ loss."],"metadata":{"id":"YcN-yIffrcl9"},"id":"YcN-yIffrcl9"},{"cell_type":"code","source":["def generator_loss_lpips(generator_image, target_image, discriminator_predictions, real_target):\n","  gen_loss = adversarial_loss(discriminator_predictions, real_target)\n","  l1_l = l1_loss(generator_image, target_image)\n","  perceptual_similarity = lpips(generator_image, target_image)\n","  result = gen_loss + (100 * l1_l) + (50 * perceptual_similarity)\n","\n","  return result"],"metadata":{"id":"UvBdirf4rAiR","executionInfo":{"status":"ok","timestamp":1674570568580,"user_tz":-60,"elapsed":942,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"UvBdirf4rAiR","execution_count":110,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"8NkWVuQ92Li0"},"id":"8NkWVuQ92Li0"},{"cell_type":"markdown","source":["Let's create objects for a new generator, a new discriminator and their Adam optimizers."],"metadata":{"id":"fWKAFOsN2Q8x"},"id":"fWKAFOsN2Q8x"},{"cell_type":"code","source":["generator_lpips = Generator(3)\n","generator_lpips.to(device)\n","generator_optimizer_lpips = op.Adam(generator_lpips.parameters(), lr=0.0002, weight_decay=0.5)\n","\n","discriminator_lpips = Discriminator(in_channels=6, kernel_size=4)\n","discriminator_lpips.to(device)\n","discriminator_optimizer_lpips = op.Adam(discriminator_lpips.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"jgWwgdbMuH0U","executionInfo":{"status":"ok","timestamp":1674570573713,"user_tz":-60,"elapsed":1547,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"jgWwgdbMuH0U","execution_count":111,"outputs":[]},{"cell_type":"markdown","source":["Therefore, the training phase would follow as:"],"metadata":{"id":"xHiejU-3rn7u"},"id":"xHiejU-3rn7u"},{"cell_type":"code","source":["generator_lpips_path = \"drive/MyDrive/models/generatorModelImageColoringLpips.pth\"\n","discriminator_lpips_path = \"drive/MyDrive/models/discriminatorModelImageColoringLpips.pth\"\n","\n","generator_lpips_losses = list()\n","discriminator_lpips_losses = list()\n","\n","num_epochs = 10\n","cnt = 1\n","\n","for epoch in range(1, num_epochs + 1):\n","  for (input_img, tg_img) in dataloader:\n","\n","    discriminator_optimizer_lpips.zero_grad()\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = generator_lpips(input_img)\n","\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","\n","    discriminator_fake = discriminator_lpips(disc_inp_fake.detach())\n","\n","    discriminator_fake_loss = discriminator_loss(discriminator_fake, fake_target)\n","\n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","\n","    discriminator_real = discriminator_lpips(disc_inp_real)\n","    discriminator_real_loss = discriminator_loss(discriminator_real,  real_target)\n","\n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    discriminator_lpips_losses.append(discriminator_total_loss.item())\n","\n","    discriminator_total_loss.backward()\n","    discriminator_optimizer_lpips.step()\n","\n","    generator_optimizer_lpips.zero_grad()\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = discriminator_lpips(fake_gen)\n","\n","    # NEW: The generator loss has changed!\n","    gen_loss = generator_loss_lpips(generated_image, tg_img, discriminator_prediction, real_target)                              \n","\n","    generator_lpips_losses.append(gen_loss.item())\n","\n","    gen_loss.backward()\n","    generator_optimizer_lpips.step()\n","\n","    if cnt % 10 == 0:\n","      print(\"Number of processed batches (batch_size = 1): \", cnt)\n","      save_model(generator_lpips, generator_lpips_path)\n","      save_model(discriminator_lpips, discriminator_lpips_path)\n","    \n","    cnt = cnt + 1"],"metadata":{"id":"8z-8cpfjrxyn"},"id":"8z-8cpfjrxyn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plots for the generator and discriminator loss are the following:"],"metadata":{"id":"d8QpoIWUMign"},"id":"d8QpoIWUMign"},{"cell_type":"code","source":["plot_gen_loss(generator_lpips_losses)\n","plot_disc_loss(discriminator_lpips_losses)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"id":"bwxZE15hMiB6","executionInfo":{"status":"ok","timestamp":1674570907526,"user_tz":-60,"elapsed":1233,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}},"outputId":"ca505da2-a59b-4d7a-a437-d79f4bb7e333"},"id":"bwxZE15hMiB6","execution_count":117,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9dX48c/JvhBIQsIWIMOOInuCgktVXJ+6UvcNl6d2catL7fL019o+S2vVWrdatVaxLtUqCO5a9wUREnZkJ4EECAkQyEL28/vj3miMgUwgM3eW83695pWZO3fuPZnAmTvnfu/5iqpijDEmesR4HYAxxpjgssRvjDFRxhK/McZEGUv8xhgTZSzxG2NMlLHEb4wxUcYSvzEhSESKROQkr+MwkckSvwlJInKRiCwQkRoR2eHe/7GIiNextSciH4jIf3odhzH+ssRvQo6I3ArcB9wF9AP6Aj8EjgYSghxLXIC3LyJi/w9NUNk/OBNSRKQX8Dvgx6r6oqpWqWOxql6qqvXueokicreIbBaRMhH5q4gku88dLyIlInKr+21hm4hc1WYf/rz2ZyKyHXhCRDJE5FURKReR3e79ge76/wscCzwoItUi8qC7fJqILBSRPe7PaW32/4GI/K+IfArUAkM7eU8SReTPIrLVvf1ZRBLd57LceCpFZJeIfNz6QeL+DqUiUiUia0Rkejf9mUyYs8RvQs1UIBGY28l6fwBGAhOA4UAO8Os2z/cDernLrwEeEpGMLrw2E8gFrsX5f/KE+3gwsA94EEBV/wv4GLheVXuo6vUikgm8BtwP9Ab+BLwmIr3b7ONyd9tpQHEnv+t/AUe58Y4HpgC/cp+7FSgBsnG+Gf0SUBEZBVwP5KtqGnAqUNTJfky0UFW72S1kbsBlwPZ2yz4DKnES7nGAADXAsDbrTAU2ufePd9eNa/P8Dpzk6c9rG4CkA8Q4Adjd5vEHwH+2eXw58EW718wHrmyz/u86eR+KgJPc+xuA/2jz3KlAkXv/dzgfksPbvX64+zufBMR7/Xe1W2jd7IjfhJqdQFbb2rqqTlPVdPe5GJyj2xSgwC1xVAJvusu/2o6qNrV5XAv08PO15apa1/pARFJE5BERKRaRvcBHQLqIxO7ndxjAt4/ii3G+WbTacuC34YDbK3aXgXMeZD3wtohsFJGfA6jqeuAnwB3ADhH5p4gMwBis1GNCz3ygHjj7AOtU4BzRj1HVdPfWS1V7+LF9f17bvmXtrcAo4EhV7YnzrQOcbw8drb8VpyzU1mCg9AD7OJD22xvsLkOdcyC3qupQ4CzgltZavqo+q6rHuK9V4M4u7NNEMEv8JqSoaiXwW+AvInKeiKSJSIyITABS3XVagMeAe0WkD4CI5IjIqX5s/2Bem4bzYVHp1u9/0+75Mr55gvZ1YKSIXCIicSJyIXA48Gqnb0DHngN+JSLZIpKFcz7iaTf2M0RkuDvMdQ/QDLSIyCgROdE9CVznxt9ykPs3EcYSvwk5qvpH4BbgdpykWgY8AvwMp96Pe3898Llbfvk3zlG5P7r62j8DyTjfFj7HKQ21dR9wnjvi535V3QmcgfNNYaf7e5yhqhV+xtfe/wCLgGXAcqDQXQYwwo2/Gufb0l9U9X2cE+R/cGPeDvQBfnGQ+zcRRlRtIhZjjIkmdsRvjDFRxhK/McZEGUv8xhgTZSzxG2NMlAloA6rukpWVpT6fz+swjDEmrBQUFFSoanb75WGR+H0+H4sWLfI6DGOMCSsi0mEfKCv1GGNMlLHEb4wxUcYSvzHGRBlL/MYYE2Us8RtjTJSxxG+MMVHGEr8xxkQZS/xR5M0V29lUUeN1GMYYj1nijxLb99Tx42cK+H8vr/A6FGOMxyzxR4m5S0ppUfhkfQXrd1R5HY4xxkOW+KOAqjK7sJSRfXuQEBvDU/M7vIrbGBMlLPFHgVXb9rKmrIrLp/o4Y1x/Xioooaqu0euwjDEescQfBWYXlhIfK5w5rj8zp/moaWjmpYISr8MyxnjEEn+Ea2puYe6SrUwf3Zf0lATGD0pn/KB0nppfTEuLzbdsTDSyxB/hPl5fQUV1PedOyvlq2ZXTctlYUcMn6ys8jMwY4xVL/BFudmEp6SnxnDCqz1fL/mNsf7J6JPDU/CLP4jLGeCegiV9EbhaRlSKyQkSeE5EkERkiIgtEZL2IPC8iCYGMIZpV1TXy9srtnDV+AAlxX/+pE+NiuXjKYN5dvYPNO2s9jNAY44WAJX4RyQFuBPJU9QggFrgIuBO4V1WHA7uBawIVQ7R7Y/l26ptaOHdizreeu+TIwcSI8PQCG9ppTLQJdKknDkgWkTggBdgGnAi86D4/CzgnwDFErZcKSxialcqEQenfeq5/r2ROG9OP5xduYV9DswfRGWO8ErDEr6qlwN3AZpyEvwcoACpVtcldrQT49uEoICLXisgiEVlUXl4eqDAjVsnuWhZs2sWMSTmISIfrXDE1lz37Gpm7pDTI0RljvBTIUk8GcDYwBBgApAKn+ft6VX1UVfNUNS87+1uTxJtOvLzYSeZnT+jwcxWAKUMyGd0vjSc/K0LVhnYaEy0CWeo5CdikquWq2gjMBo4G0t3SD8BAwA43u1lri4Yjh2QyKDNlv+uJCFdO87F6exULi3YHMUJjjJcCmfg3A0eJSIo4tYbpwCrgfeA8d52ZwNwAxhCVlmypZGNFDd+bNLDTdc+ekEOv5HhmfVYU+MCMMSEhkDX+BTgncQuB5e6+HgV+BtwiIuuB3sDjgYohWs1ZXEpiXAynj+3X6brJCbFcmD+IN1duZ9uefUGIzhjjtYCO6lHV36jqaFU9QlUvV9V6Vd2oqlNUdbiqnq+q9YGMIdo0NLUwb+lWThnTj7SkeL9ec9mRubSo8uyCzQGOzhgTCuzK3Qjz/podVNY2MmPS/k/qtje4dwrTR/fhuS82U99kQzuNiXSW+CPMnMJSsnokcuzwrC697oqpPiqqG3h9+bYARWaMCRWW+CNIZW0D764u4+wJA4iL7dqf9pjhWQzNTuXJz+xKXmMinSX+CPLKsm00NmuXyjytYmKEK47KZemWSpZsqQxAdMaYUGGJP4LMKSxhVN80Du/f86Be/73JA0lNiOUpG9ppTESzxB8hNlXUULi58oAtGjqTlhTPeZMH8uqybVRU22ArYyKVJf4IMaewhBiBczroxNkVl0/10dDcwj+/sKGdxkQqS/wRoKVFmb24lKOHZ9G3Z9IhbWt4nx4cOyKLpz/fTFNzSzdFaIwJJZb4I8Ci4t2U7N53UCd1O3LFVB/b99bx9qqybtmeMSa0WOKPALMLS0hJiOXUMZ23aPDHiaP7MDAj2fr3GBOhLPGHubrGZl5bvo3TjuhHSkJc5y/wQ2yMcPlRuSzYtIsvt+3tlm0aY0KHJf4w9+8vy6iqa/KrE2dXXJg/iMS4GJuQ3ZgIZIk/zM0uLKV/rySOGtq7W7ebnpLAORNymLO4lD21jd26bWOMtyzxh7GK6no+XFvO2RNyiI05uLH7B3LFtFzqGlt4YdGWbt+2McY7lvjD2LwlW2luObgWDf4YM6AX+b4M/vF5Mc0tNjWjMZHCEn8Ym724hLE5vRjZNy1g+5g5zcfmXbV8sGZHwPZhjAkuS/xham1ZFStK9wbsaL/VqWP60bdnIrPmW9dOYyKFJf4wNbuwlNgY4czxAwK6n/jYGC49MpeP1pazobw6oPsyxgSHJf4w1NyivLy4lONHZpPVIzHg+7t4ymDiY4V/2FG/MRHBEn8Ymr9hJ9v31jGjm8fu7092WiLfHdufFwtKqK5vCso+jTGBY4k/DM1eXEJaUhzTD+sTtH3OnOajur6JOYUlQdunMSYwLPGHmZr6Jt5csZ0zxvUnKT42aPudMCidcQN7MWt+Mao2tNOYcGaJP8y8tXI7tQ3NQSvztBIRZk71sX5HNZ9t2BnUfRtjupcl/jAzZ3EpgzKTycvNCPq+vzuuP5mpCTxpXTuNCWuW+MPI9j11fLK+gnMnDjzo6RUPRVJ8LBdPGcS7X5axZVdt0PdvjOkelvjDyMtLSlGFGYc4veKhuPTIXESEpxfY0E5jwpUl/jChqswuLGHS4HR8WamexTEgPZlTDu/L8wu3UNfY7FkcxpiDZ4k/TKzcupe1ZdVBP6nbkZnTfFTWNjJvyVavQzHGHARL/GFidmEpCbExnDGuv9ehcOSQTEb1TePJz4psaKcxYcgSfxhoam5h3tJSThzdh/SUBK/DcYZ2TvOxatteCop3ex2OMaaLApb4RWSUiCxpc9srIj8RkTtEpLTN8v8IVAyR4uN1FVRUNwS8E2dXnDNxAD2T4mxopzFhKGCJX1XXqOoEVZ0ATAZqgTnu0/e2PqeqrwcqhkjxUmEJGSnxHD8qeC0aOpOSEMcFeYN4c8V2yvbWeR2OMaYLglXqmQ5sUFUbA9hFe+saeXtVGWeOH0BCXGhV5i6fmkuzKs8s2Ox1KMaYLghWJrkIeK7N4+tFZJmI/F1EOrwEVUSuFZFFIrKovLw8OFGGoDeWb6OhqSUkRvO0l9s7lRNG9eHZBZtpaGrxOhxjjJ8CnvhFJAE4C/iXu+hhYBgwAdgG3NPR61T1UVXNU9W87OzsQIcZsl4qLGVodirjB/byOpQOXTE1l4rqet5Ysc3rUIwxfgrGEf/pQKGqlgGoapmqNqtqC/AYMCUIMYSlLbtq+WLTLmZMzPGkRYM/jhuRzZCsVGbZSV5ziHZW13PTPxfzwqItXocS8YKR+C+mTZlHRNoORD8XWBGEGMLSy4tLATjHwxYNnYmJES4/KpfCzZUsL9njdTgmTC3evJszHviEuUu28tT8Iq/DiXgBTfwikgqcDMxus/iPIrJcRJYBJwA3BzKGcKWqzF5cylFDMxmYkeJ1OAd0Xt5AUhJimTW/yOtQTJhRVf4xv4gLHplPbIzw3bH9WbV1r830FmABTfyqWqOqvVV1T5tll6vqWFUdp6pnqaoVhzuweEslmypqmDEx9E7qttczKZ7vTRrIvKVb2Vld73U4Jkzsa2jmlheW8v/mruSY4Vm8esMxXJg/iBZ1vgGYwAmt8YHmK3MKS0mMi+H0sf28DsUvV0zNpaGpheetPmv8sKmihnP/8ikvLynllpNH8vjMfNJTEpg4OJ0YgUVFlvgDyRJ/CKpvauaVZVs5dUw/0pLivQ7HLyP6pnH08N48Pb+YpmYb2mn27+2V2znrgU/YvreOJ6+awo3TRxAT4wxeSEuKZ3S/niwq3uVxlJHNEn8Ien91OZW1jZwbQi0a/HHFVB9b99Tx7y/LvA7FhKCm5hbufHM11/6jgCHZqbx6wzF8Z+S3h2rn+zJYvLmSRjuACBhL/CFozuISsnokcuzwLK9D6ZKTDutLTnoysz6zC7TNN1VU13PF37/g4Q82cPGUwbzwg6n7HbSQ58uktqGZL7ftDXKU0cMSf4jZXdPAe6t3cM6EAcTFhtefJzZGuOyoXOZv3Mma7VVeh2NCROHm3Zxx/ycUFO/mj+eN4/czxpIUH7vf9fN8zsX8VucPnPDKLFHg1WVbaWzWkGzR4I+L8geRGBdjY7ENqspT84u48JH5xMcJL/1oGhfkDer0df17JZOTnmx1/gCyxB9iZi8uZXS/NA4f0NPrUA5KRmoCZ40fwOzCUvbsa/Q6HOOR2oYmbn5+Cb+eu5JjR2Tz6vXHckSO/21H8n0ZLCzabRP9BIgl/hCysbyaxZsrQ6rv/sGYOc3HvsZmXiwo8ToU44FNFTWc+9BnzF26lVtPHsnfrsijV0rXRqfl+TIpr6pny659AYoyunWa+EXkaPcKXETkMhH5k4jkBj606DNncSkxAmdPCO/Ef0ROL/JyM/jH/CJaWuyILZq85Q7V3FFVx6yrpnBDm6GaXdFa519YZOWeQPDniP9hoFZExgO3AhuApwIaVRRqaVHmLC7l6OFZ9O2Z5HU4h+yKaT6Kdtby4brobakdTZqaW/jDG6v5gTtU85UbjuG4DoZq+mtknzTSkuKszh8g/iT+JnUKbWcDD6rqQ0BaYMOKPguLdlGyex/fC9OTuu2dNqYffdISrWtnFCivqufyx7/grx9u4JIjB/OvH+5/qKa/YmKEvFynzm+6nz+Jv0pEfgFcBrwmIjFAeFxOGkZmF5aSmhDLKWP6eh1Kt0iIi+GSIwfzwZpyiipqvA7HBEhB8W7OeOBjCjfv5q7zxvF/544lMW7/QzW7Is+Xyfod1eyuaeiW7Zmv+ZP4LwTqgWtUdTswELgroFFFmbrGZl5fvo3TjuhPSkKc1+F0m0uOHEx8rPDUfLugK9KoKrM+K+KiR+eTGBfL7B9P43w/hmp2RV6uU+cvKLaj/u7m1xE/cJ+qfiwiI3Fmznquk9eYLnhnVRlV9U18L8xH87TXJy2J04/oz78KtlBjbXYjRm1DEz95fgm/mbeS40Zk88r1xzBmQPfPEDd+UDrxscJCq/N3O38S/0dAoojkAG8DlwNPBjKoaDO7sIQBvZI4amhvr0PpdjOn+aiqa2KOO6mMCW8by6s556FPmbd0K7edMpLHDmKopr+S4mMZm9OLAqvzdzt/Er+oai0wA/iLqp4PHBHYsKJHeVU9H62r4OyJOQc17C3UTRqczhE5PXlqfpFdjBPm3lyxjbMe/JTyqnqeunoK1594cEM1uyLPl8mykj3UNTYHdD/Rxq/ELyJTgUuB17rwOuOHeUu30tyizAjh6RUPhYgwc6qPtWXVzN+40+twzEFoam7h969/yQ+fLmRYdiqv3ngsx444+KGaXZGXm0FDcwvLS21az+7kTwL/CfALYI6qrhSRocD7gQ0reswuLGHcwF6M6Bu5I2TPHD+AjJR4G9oZhsqr6rns8QU88tFGLj1yMC/8cCo56clB2//kXGvYFgidDiFR1Q+BD0Wkh4j0UNWNwI2BDy3yrdlexcqte/nNmYd7HUpAJcXHctGUwTzy4QZKK/cFNXGYg1dQvIsfP1NIZW0j95w/nu9NDv41Jr17JDIsO5VFRbuAYUHff6Typ2XDWBFZDKwEVolIgYiMCXxokW/24hLiYoQzxw/wOpSAu+wop8vH05/b0M5Qp6o88ekmLnzkc5LiY5nz46M9Sfqt8nIzWVS829p/dCN/Sj2PALeoaq6qDsZp2/BYYMOKfM0tysuLSzl+VDZZPRK9DifgctKTOfnwvvzzi812oi6E1dQ3cdM/l/DbV1Zx/Khs5l1/jOedYvN8GezZ18j68mpP44gk/iT+VFX9qqavqh8AqQGLKEp8tqGCsr31nDsxMlo0+GPmNB+7axt5ZelWr0MxHdjgDtV8ddlWfnrqKB69PI9eyd5fpJ/vywSszt+d/En8G0Xk/4mIz739CtgY6MAi3ZzCUtKS4ph+WB+vQwmaqUN7M7JvD2bNt6Gdoeb91Ts4+8FP2VnTwFNXH8l1JwwPmeHFub1TyOqR4Nb5TXfwJ/FfDWQDs91btrvMHKSa+ibeWLGdM8YNOOAUdJFGRLhiqo8VpXtZsMn+E4eKDeXVXP9sIYMzU3j1hmM4ZkRozfUsIuTlZtoVvN2o08SvqrtV9UZVneTeblJV+851CN5csZ19jc1hP+HKwZgxKYec9GR++uJS9tbZDF1eq2ts5rpnCkmIi+FvM/MYEKIjrvJ8GWzZtY+yvXVehxIR9pv4ReQVEZm3v1swg4w0cxaXMigz+asmVNEkJSGO+y+ewNbKOn4xe7mVfDz221dWsXp7FX+6YELIJn1wruAFq/N3lwON4787aFFEkW179vHphgpuPHEEIqFRQw22ybmZ3HbKKO58czXThvXm0iNtQjcvzF1SynNfbOaH3xnGCaND+1zTmAE9SYqPYWHRLr47rr/X4YS9/SZ+98It081eXrwVVTg3Qls0+OsHxw1l/sad/O6VVUwanMFh/cNzcvlwtaG8ml/OXk5ebga3njLS63A6FR8bw8RBGdaiuZtYz50gUlVmF5YwOTcDX1Z0j4iNiRH+dMF4eibHc/2zhdQ2WNvmYGlb17//4onEx4ZHGsj3ZbBy6x6qrcX3IQuPv3iEWLl1L+t2VEflSd2OZPVI5L4LJ7CxooZfz13pdThRI1zq+u1N9mXSorBkc6XXoYS9AyZ+EYkVkYOq9YvIKBFZ0ua2V0R+IiKZIvKOiKxzf0bNGc6XCktIiI3hjLGR36LBX9OGZ3HDiSN4saCE2YUlXocT8cKprt/epMHpxIgzP7U5NAdM/KraDBxzMBtW1TWqOkFVJwCTgVpgDvBz4F1VHQG86z6OeI3NLcxbspXph/UJ2MQV4erGE4czZUgmv3p5BRvssvyA2Rhmdf320pLiGd2vp9X5u4E/pZ7F7hDOy0VkRuuti/uZDmxQ1WLgbGCWu3wWcE4XtxWWPl5Xzs6aBmZMip4WDf6Ki43h/osmkhgXw3XPFFovnwCoa2zmumcXh11dv708XwaFm3fT1NzidShhzZ+/fhKwEzgRONO9ndHF/VzE1/P09lXVbe797UDfjl4gIteKyCIRWVReXt7F3YWelwpLyUxN4DsjgzOBRbjp1yuJP10wgdXbq/jf1770OpyI87tXV/Hltr1hV9dvL8+XSW1DM19uq/I6lLDmTz/+qw5lByKSAJyFM5lL+22riHR4BY+qPgo8CpCXlxfWV/ns2dfIO6vKuDh/EAlx4XmkFQwnjO7DtccN5dGPNjJtWG9OH2vjtbvD3CWlPLsgPOv67eX73IlZincxdmD3T/AeLfzpxz9QROaIyA739pKIdKVecTpQqKpl7uMyEenvbrs/sKPrYYeXN5Zvo6Gpxco8frjtlFGMH5TO7S8tY8uuWq/DCXvhXtdvr3+vZHLSk+0K3kPkz+HnE8A8YIB7e8Vd5q+L+brMg7utme79mcDcLmwr7Kgqz36xmaHZqYyzI5ROJcTF8ODFEwG4/rnFNDRZLfdgRUpdv708XwYLi3ZZu49D4M+/hGxVfUJVm9zbkzgdOjslIqnAyThdPVv9AThZRNYBJ7mPI9ZbK8tYVrKHHxw3NGpbNHTVoMwU/vi9cSzdUsndb6/xOpywFSl1/fbyfJnsqKpny659XocStvxJ/DtF5DJ3TH+siFyGc7K3U6pao6q9VXVPm2U7VXW6qo5Q1ZNUNWIH5TY1t3DXW6sZlp3K96zM0yWnj+3P5Ufl8uhHG3lvdVnnLzDfEEl1/fba1vnNwfG3H/8FOCNwtgHnAVcGMKaI8VJhCRvKa/jpqaOJi5Cv2cH0X989jMP69+TWF5aybY8d3fkr0ur67Y3sk0ZaUhwLrc5/0PzJRgNV9SxVzVbVPqp6DjA40IGFu7rGZu59Zx0TBqVz6pgOR6yaTiTFx/LgJROpb2rhpn8usbHbfojUun5bMTHC5NwMm5HrEPjzr+IBP5eZNmZ9VsT2vXX87LTRVts/BMOye/A/5xzBF5t2cf97670OJ+RFal2/vXxfJut2VFNZ2+B1KGFpv+P4RWQqMA3IFpFb2jzVE4ie+QIPwp59jfzlgw18Z2Q2U4f19jqcsDdj0kA+27CTB95bx1FDMpk2PLSmBgwVkVzXb2+yO4lRQfFuph9m36i76kBH/AlAD5wPh7Q2t704dX6zH498uIE9+xq5/bRRXocSMX539hiGZqVy0/NLqKiu9zqckBPpdf32xg9MJz5WrM5/kDqbiOVDEXnS7bFj/FC2t46/f7qJsycMYMwAG7ffXVIS4njwkkmc89Cn3Pz8EmZdNYWYGCuhQXTU9dtLTojliJxeFNjInoPiz7+QWhG5S0ReF5H3Wm8BjyxM3ffuOpqalVtOjvyjrmA7rH9Pfn3m4Xy8roJHPtrodTghI1rq+u3l+zJZumWPNfU7CP4k/meA1cAQ4LdAEbAwgDGFrU0VNTy/cAuXHDmY3N7RPcNWoFwyZTDfHdefu99eY0d7RFddv73JuRk0NLewonRP5yubb/An8fdW1ceBRlX9UFWvxunUadq5++01JMbFcMOJI7wOJWKJCL+fMZac9GRufG5JVI/qiLa6fnt57gleq/N3nT+Jv9H9uU1EvisiE4HMAMYUlpaX7OG1Zdv4z2OGkJ2W6HU4Ea1nUjwPXDyRHVV13P7isqjs2RKNdf32evdIZGh2qn3zOwj+/Gv5HxHpBdwK3Ab8Dbg5oFGFoTvfXE1GSjzfP26o16FEhfGD0vnZaaN5e1UZsz4r8jqcoIvWun57ebkZLCreTUtL9H34H4pO59wFRqjqHlVdoaonqOpkVZ0XpPjCwifrKvhkfQXXnTCctCSbVjFYrjlmCNNH9+H/Xl8dVXXe1rr+D74zNOrq+u3l+TKprG20KTu7yJ85dy8OUixhSVX541uryUlP5rKjcr0OJ6qICHefP57ePRK4/tlCquoaO39RmGut60/OzeC2U+w6kXyfU3VeZPPwdok/pZ5PReRBETlWRCa13gIeWZh4ffl2lpXs4eaTR5IUbxc0B1tGagL3XzyRLbv38V9zVkR0vb+1rh8fF8MDUVrXb8/XO4WsHgkstL49XdLp1IvABPfn79osU2xkD43NLdz99hpG9u3BuRNzvA4nauX7Mrn5pBHc/fZajh7emwvzI7OHYGtd/+9X5kV1Xb8tkdaGbXbE3xX+zLl7QjACCUf/WlTCpooaHrsij1i7itRTPzp+OJ9v3MVv5q1k4uAMRvZN8zqkbjVv6dav6vonjrbeNG3l+zJ5a2UZO/bW0adnktfhhAV/5tztKyKPi8gb7uPDReSawIcW2vY1NPPnf68lLzeDkw6L7hNsoSA2RvjThePpkRjHdc8Usq8hcq7m3FRRwy9eWmZ1/f3Iszp/l/lTJHwSeAtnvl2AtcBPAhVQuHjis03sqKrnZ6db2+VQ0SctiT9fOJH15dXcMW+l1+F0i7rGZn78TKHV9Q9gzICeJMXHWJ2/C/z5V5Slqi8ALQCq2gREzuHUQaisbeDhDzYwfXSfr0YVmNBwzIgsfnz8MJ5ftIW5S0q9DueQ/fdX4/XHW11/P+JjY5gwKN3q/F3gT+KvEZHeOCd0EZGjgOgZNN2Bhz/cQHV9Ez+1tssh6eaTRpKXm8EvZy9nU0WN1+EctHlLt/KM1fX9ku/LZNW2vdTUN3kdSljwJ/HfAswDhonIp8BTwA0BjSqEbduzjyc/LeLcCTmM7oyxb2QAABj0SURBVNfT63BMB+Ji3TYGcTFc/2wh9U3h9wXV6vpdMzk3g+YWZcmWSq9DCQudJn5VLQS+gzMb1w+AMaq6LNCBhar7/r0OVbjZ2i6HtAHpydx13nhWbt3L719f7XU4XWJ1/a6blJuBCFbn95O//6KmAOOBScDFInJF4EIKXet3VPPCoi1cetRgBmWmeB2O6cTJh/fl6qOH8ORnRby5YrvX4fjN6vpd1zMpntH9elJgI3v84s9wzn8AdwPHAPnuLS/AcYWke95eQ3J8LNedMNzrUIyffnb6KMbm9OL2F5dSsrvW63A6ZXX9g5fvy6CweDdNzS1ehxLy/DnizwOOVtUfq+oN7u3GQAcWapZsqeSNFdv5/nFDyephbZfDRWJcLA9eMpEWhRueW0xjCCcFq+sfmsm5GdQ0NLN6e5XXoYQ8fxL/CqBfoAMJZarKnW+spndqAv95rLVdDje5vVP5/YyxLN5cyT1vr/U6nA5ZXf/QtQ6ttjp/5/waxw+sEpG3RGRe6y3QgYWSj9ZVMH/jTm44cTg9Ev1pb2RCzZnjB3DxlMH89cMNfLBmh9fhfIvV9Q/dgPRkctKT7QpeP/iTxe4IdBChrKXFOdofmJHMxUdGZvOvaPGbMw+nsHg3t76wlNdvOpa+IdLXxer63WdybgYLNu1EVe2K+gPwp0nbhyKSizMhy79FJAWImv7Dry7fxqpte7n3wvEkxkXNrx2RkuKdev9ZD37KT/65hKf/88iANddrbG5hX2Mz+xqaqW1wfu5rbGJfQwu1DU1fPVdd38S976y1un43yfdlMG/pVkp277ORdwfQaeIXke8D1+LMszsMyAH+Ckz347XpOFM1HoFz5e/VwKnA94Fyd7VfqurrBxN8oDU0tXDP22sY3S+Ns8db2+VIMKJvGr89ewy3v7iM+/69lkuOzP1GIq5taP7q/r7G1qTd1Ob+18vr3J9f32/66vnGZv/nBejfK8nq+t3k64ZtuyzxH4A/pZ7rcMbxLwBQ1XUi4m87yvuAN1X1PBFJAFJwEv+9qnr3wQQcTM8v3EzxzlqeuDKfGGu7HDHOnzyQ+Rt2cv9767n/vfV+vSY2RkiJjyU5wb2591MSYslISXDut3n+G/e/Wj+O5Hj3sbssJSGWHolxxFnS7xYj+6aRlhTHwqLdnDtxoNfhhCx/En+9qja01stEJA63b8+BuBO0HwdcCaCqDUBDuNTdauqbuO/d9UzxZXL8qGyvwzHdSET4v3PHMnVob5palJSEWJLcJNz2vpPA40hOiCU+VqxmHAZiY4RJgzNYZCN7DsifxP+hiPwSSBaRk4EfA6/48bohOOWcJ0RkPFAA3OQ+d7179e8i4FZV/dZpeBG5FqfExODBwT+p+sSnm6iorueRyyfbf/gIlJwQywX5g7wOwwRAvi+Du98uZ09tI71S4r0OJyT58/3y5zgJfDlOr57XgV/58bo4nBYPD6vqRKDG3dbDOOcKJgDbgHs6erGqPqqqeaqal50d3CPu3TUNPPLhRk4+vC+TczOCum9jzKFprfMXbLaj/v3xp0lbi6o+pqrnq+p57n1/zlyVACWqusB9/CIwSVXLVLVZVVuAx3DOH4SUh95fT01DEz891UZZGBNuxg9MJy5GWGj9+fdrv4lfRM4WkevaPF4gIhvd2/mdbVhVtwNbRKQ1e07HuRCsf5vVzsW5MjhklFbu46n5xcyYNDDi5m01JhokJ8RyRE4vq/MfwIFq/LcDF7V5nIjToC0VeAL4lx/bvwF4xh3RsxG4CrhfRCbgnCAuwikfhYw/v7MWxNouGxPO8n0ZzJpfTH1Ts11/04EDJf4EVd3S5vEnqroT2Ckiqf5sXFWX8O1Onpd3McagWVdWxUuFJVx99BBy7LJ5Y8LW5NxMHvt4EytK9zA516ZHbe9ANf5vnNVU1evbPIzI8Y1/fGsNqQlx1nbZmDCX53PSl9X5O3agxL/AvWr3G0TkB8AXgQvJGwXFu3hnVRnXHjeUjNQEr8MxxhyCrB6JDM1KtQnY9+NApZ6bgZdF5BKg0F02GafWf06gAwsmp+3yGrJ6JHLNsUO8DscY0w3yfBm8s6qMlha1K+/b2e8Rv6ruUNVpwH/jnIQtAn6nqlNVtSw44QXHB2vK+aJoFzdNH05KgrVdNiYS5OVmsru2kY0V1V6HEnL86c75HvBeEGLxREuLcuebq8ntncJFU6ztsjGRorXOv6hoN8P72NDstqK+M9TcpaWs3l7FLSePtO6IxkSQIVmp9E5NsBO8HYjqTOe0XV7LmAE9OXPcAK/DMcZ0IxFhcm4Gi4rtQq72ojrxP7ugmJLd+7j9tNF28seYCJTvy6R4Zy07quq8DiWkRG3ir65v4oH31jN1aG+OG5HldTjGmABorfMXWLnnG6I28f/t443srGng9tNGWdtlYyLUmAG9SIqPsTp/O1GZ+Cuq63nso42cNqYfEwdb22VjIlVCXAzjB6Zbnb+dqEz8D72/nn2NzdxmbZeNiXj5vkxWbt1LbUOT16GEjKhL/Ft21fLM55u5IG8Qw/v08DocY0yA5fkyaG5Rlmyu9DqUkBF1if/ed9YiAjedNMLrUIwxQTApNwMRa9jWVlQl/i+37WXOklKunOajfy9ru2xMNOiZFM+ovmlW528jqhL/3W+tIS0xjh8dP8zrUIwxQZTvy6SweDdNzS1ehxISoibxLyzaxburd/DD44eRnmJtl42JJnm+DGoamlm9vcrrUEJCVCR+VeUPb6ymT1oiV02ztsvGRJs8nzMLl83D64iKxP/vL3dQULybm04aQXKCzb9pTLTJSU9mQK8kFhXbCV6IgsTf3KLc9dZqhmSlckHeIK/DMcZ4JM+XycKiXaiq16F4LuIT/5zFpawtq+a2U0ZZ22VjolieL4OyvfWU7N7ndSiei+hMWNfYzL3vrGVsTi9OP6Kf1+EYYzyUl+vU+Qus3BPZif/pz4sprdzHz6ztsjFRb1S/NNIS41hoJ3gjO/EnJ8Ty3XH9OcbaLhsT9WJjhIm5GSyyK3gjO/FfemQuD10yyeswjDEhIj83gzVlVeypbfQ6FE9FdOI3xpi2WsfzF26O7qN+S/zGmKgxYVA6cTESFnX+fQ3N/GbuCiprG7p925b4jTFRIzkhljE5vUK+zt/U3ML1zxbyj8+LWbKl+9tJW+I3xkSV/NwMlpZUUt/U7HUoHVJV/mvOCt5dvYPfnX0Ex4/q0+37sMRvjIkqeb5M6ptaWFG61+tQOnTvO2t5ftEWbjxxOJcdlRuQfQQ08YtIuoi8KCKrReRLEZkqIpki8o6IrHN/2qS3xpigmZzrpJxQbNj29OfF3P/eei7MG8TNJ48M2H4CfcR/H/Cmqo4GxgNfAj8H3lXVEcC77mNjjAmK7LREhmSlhlzDtjdXbOfXc1cwfXQf/vfcIxAJ3EWnAUv8ItILOA54HEBVG1S1EjgbmOWuNgs4J1AxGGNMR/JyM1gUQg3bFhbt4sZ/Lmb8oHQevGQScQHuKxbIrQ8ByoEnRGSxiPxNRFKBvqq6zV1nO9C3oxeLyLUiskhEFpWXlwcwTGNMtMnzZbC7tpEN5TVeh8LasiqueXIhAzOS+fvM/KC0jg9k4o8DJgEPq+pEoIZ2ZR11Pm47/MhV1UdVNU9V87KzswMYpjEm2oTKxCxbK/cx8+9fkBQfy6yrppCRGpzZAQOZ+EuAElVd4D5+EeeDoExE+gO4P3cEMAZjjPmWoVmpZKYmeFrn31PbyJVPfEF1XRNPXjWFQZkpQdt3wBK/qm4HtojIKHfRdGAVMA+Y6S6bCcwNVAzGGNMREfmqzu+FusZmvv/UIooqannkiskcPqBnUPcfF+Dt3wA8IyIJwEbgKpwPmxdE5BqgGLggwDEYY8y35PkyeHtVGeVV9WSnJQZtv80tyk3/XMzC4l08cPFEpg0LfvfggCZ+VV0C5HXw1PRA7tcYYzrTWucvKN7FaUf0D8o+VZVfz13BWyvL+M2Zh3PGuAFB2W97duWuMSYqHTGgF4lxMSwMYt+eB99bzzMLNvPD7wzjqqOHBG2/7VniN8ZEpYS4GMYPSg9anf+FhVu45521zJiUw89OG9X5CwLIEr8xJmrl+zJYuXUvtQ1NAd3Pu1+W8Ys5yzluZDZ3fm9cQK/K9YclfmNM1MrzZdLUogFpfdyqcPNurnu2kDEDevLwpZOID/BVuf7wPgJjjPHIpMEZiBCw/vwbyqu55smF9O2ZxN+vzCc1MdADKf1jid8YE7V6Jcczqm9aQGbkKttbxxWPf0FsjPDU1VPI6hG8IaOdscRvjIlqeb4MFm+upLml+xq27a1r5MonFlJZ28ATV04ht3dqt227O1jiN8ZEtXxfJtX1Taze3j0Ts9Q3NfODpwpYV1bFw5dNZuzAXt2y3e5kid8YE9W+npjl0Ov8LS3KLS8sZf7Gndx1/jiOGxmaDSYt8RtjolpOejL9eyUdcsM2VeW/X1vFa8u28cv/GM25Ewd2U4TdzxK/MSaqiQh5vkwWbjq0iVke/WgjT3xaxNVHD+H7xw7txgi7nyV+Y0zUy8vNYPveOkor9x3U62cXlvD7N1Zzxrj+/Oq7h3l+gVZnLPEbY6Jenu/g6/wfri3n9heXMW1Yb+65YDwxMaGd9MESvzHGMLpfT3okxrGouGvj+ZeVVPKjpwsY0TeNRy6fTGJc4KdN7A6W+I0xUS82RpiUm9GlI/6iihquemIhmakJzLoqn7Sk+ABG2L0s8RtjDE6df01ZFXv2NXa6bnlVPTOf+IIWVWZdPYU+PZOCEGH3scRvjDE4dX5Vp6nagVTXN3H1kwvZsbeev1+Zz7DsHkGKsPtY4jfGGGDCoHTiYuSA/fkbmlr40dMFrNq2l4cuncjEwRlBjLD7WOI3xhggJSGOMQN67ndGrpYW5fYXl/Lxugp+P2MsJ47uG+QIu48lfmOMceX5Mlm6pZKGppZvPXfnm6t5eclWbjtlJBfkDfIguu5jid8YY1z5vgzqm1pYsXXPN5Y//skmHvloI5cflct1Jwz3KLruY4nfGGNck3MzAb5R55+3dCv//eoqThvTjzvOGhPyV+X6wxK/Mca4stMS8fVO+arO/9n6Cm59YQlTfJn8+aIJxIbBVbn+sMRvjDFt5PkyKSjezYrSPVz7jwKGZKXy2BV5JMWHx1W5/rDEb4wxbeT7MthV08Alj31OWlIcs66eQq+U8Lkq1x+W+I0xpo3WOj/ArKun0L9XsofRBEZoTPlujDEhYlh2KtedMIyTDuvLyL5pXocTEJb4jTGmDRHhp6eO9jqMgLJSjzHGRBlL/MYYE2UCmvhFpEhElovIEhFZ5C67Q0RK3WVLROQ/AhmDMcaYbwpGjf8EVa1ot+xeVb07CPs2xhjTjpV6jDEmygQ68SvwtogUiMi1bZZfLyLLROTvItJhQ2sRuVZEFonIovLy8gCHaYwx0SPQif8YVZ0EnA5cJyLHAQ8Dw4AJwDbgno5eqKqPqmqequZlZ2cHOExjjIkeAU38qlrq/twBzAGmqGqZqjaragvwGDAlkDEYY4z5poCd3BWRVCBGVavc+6cAvxOR/qq6zV3tXGBFZ9sqKCioEJHiQMUaJFlA+5Pc0czej6/Ze/FN9n5806G8H7kdLQzkqJ6+wBy3d3Uc8Kyqviki/xCRCTj1/yLgB51tSFXDvtYjIotUNc/rOEKFvR9fs/fim+z9+KZAvB8BS/yquhEY38HyywO1T2OMMZ2z4ZzGGBNlLPEHz6NeBxBi7P34mr0X32Tvxzd1+/shqtrd2zTGGBPC7IjfGGOijCV+Y4yJMpb4A0xEBonI+yKySkRWishNXsfkNRGJFZHFIvKq17F4TUTSReRFEVktIl+KyFSvY/KKiNzs/h9ZISLPiUiS1zEFk9vCZoeIrGizLFNE3hGRde7PDlvcdJUl/sBrAm5V1cOBo3BaVxzucUxeuwn40usgQsR9wJuqOhpn+HNUvi8ikgPcCOSp6hFALHCRt1EF3ZPAae2W/Rx4V1VHAO+6jw+ZJf4AU9Vtqlro3q/C+Y+d421U3hGRgcB3gb95HYvXRKQXcBzwOICqNqhqpbdReSoOSBaROCAF2OpxPEGlqh8Bu9otPhuY5d6fBZzTHfuyxB9EIuIDJgILvI3EU38GbgdavA4kBAwByoEn3NLX39z2JlHH7et1N7AZp3njHlV929uoQkLfNi1utuN0RDhklviDRER6AC8BP1HVvV7H4wUROQPYoaoFXscSIuKAScDDqjoRqKGbvsqHG7d2fTbOh+EAIFVELvM2qtCiztj7bhl/b4k/CEQkHifpP6Oqs72Ox0NHA2eJSBHwT+BEEXna25A8VQKUqGrrN8AXcT4IotFJwCZVLVfVRmA2MM3jmEJBmYj0B3B/7uiOjVriDzBxutQ9Dnypqn/yOh4vqeovVHWgqvpwTty9p6pRe1SnqtuBLSIyyl00HVjlYUhe2gwcJSIp7v+Z6UTpie525gEz3fszgbndsVFL/IF3NHA5ztGtTTBv2rsBeEZEluFMTvR/HsfjCfdbz4tAIbAcJzdFVesGEXkOmA+MEpESEbkG+ANwsoisw/lW9Idu2Ze1bDDGmOhiR/zGGBNlLPEbY0yUscRvjDFRxhK/McZEGUv8xhgTZSzxm/0SERWRe9o8vk1E7uimbT8pIud1x7Y62c/5btfL99stP76r3UFF5CciktLJOneIyG0HE2ukCtbf2vjPEr85kHpghohkeR1IW24TL39dA3xfVU/ohl3/BKd5WMjp4ntiopwlfnMgTTgX0dzc/on2R3EiUu3+PF5EPhSRuSKyUUT+ICKXisgXIrJcRIa12cxJIrJIRNa6fXxae/XfJSILRWSZiPygzXY/FpF5dHB1q4hc7G5/hYjc6S77NXAM8LiI3NXB79dTRF4TkTUi8lcRiXFf97Ab10oR+a277EacHjLvt357EJHTRKRQRJaKyLtttnu4iHzg/v43tonxMvd9WCIij7i/a6z7Xq5w49/fe/3XDt6rK0Vknoi8B7zr9m5/2X3fPheRce56PUTkCXf7y0Tke+7yU0Rkvvs7/MvtJ4X7N1vlrnu3u+x8N8alIvJRJ38rEZEH3ff130CfDt574yVVtZvdOrwB1UBPoAjoBdwG3OE+9yRwXtt13Z/HA5VAfyARKAV+6z53E/DnNq9/E+fgYwRO35ok4FrgV+46icAinMZdx+M0MRvSQZwDcC75z8ZpfPYecI773Ac4Pd7bv+Z4oA4YitP7/Z3W3wfIdH/Guq8f5z4uArLc+9nAltZ42rzmDuAzN/YsYCcQDxwGvALEu+v9BbgCmAy80yau9A5i3d97daV7v3XfDwC/ce+fCCxx79/Z+r67jzPc2D4CUt1lPwN+DfQG1vD1xZ3p7s/lQE67Zfv7W81w389Y929TSZt/K3bz/mZH/OaA1Okk+hTOJBn+WqjOPAT1wAagtb3ucsDXZr0XVLVFVdcBG4HRwCnAFSKyBKd9dW+cZAfwhapu6mB/+cAH6jT4agKewelz35kvVHWjqjYDz+F8OwC4QEQKgcXAGKCjiXOOAj5qjUdV2/ZRf01V61W1AqepVl+c3jOTgYXu7zYd50NnIzBURB4QkdOA/XVu7ei9AudDo3XfxwD/cON5D+gtIj1xLvV/qHVDqrrbjf9w4FM3nplALrAH5wPxcRGZAdS6L/sUeFJEvo+T0GH/f6vjgOdUtVlVt+J8EJsQYnVB448/4/RQeaLNsibcUqFbIklo81x9m/stbR638M1/c+37hSggwA2q+lbbJ0TkeJwj/u70rf2LyBCcbzb5qrpbRJ7EObruira/fzPO7yzALFX9RfuVRWQ8cCrwQ+AC4Gp/YnV/Hux7IjgfGhd3EM8UnA+m84DrgRNV9YciciTOJDoFIjKZ/f+trBdViLMjftMp94jyBZwTpa2KcI5gAc7CKWd01fkiEuPW/YfilBjeAn4kTitrRGSkdD45yRfAd0QkS0RigYuBD/3Y/xQRGeJ+cF0IfIJT2qoB9ohIX+D0NutXAWnu/c+B49wPCkQks5N9vQucJyJ9WtcXkVxxTpzHqOpLwK/Yf1vmjt6r9j4GLnW3fzxQ4X5jewe4rnUlcXrffw4cLSLD3WWp7nvdA+ilqq/jnNsZ7z4/TFUXqOqvcSaPGcT+/1YfARe65wD6A91xYt10IzviN/66B+for9VjwFwRWYpTfz6YI8/NOEm7J/BDVa0Tkb/hlIMKRURwkswBp5tT1W0i8nPgfZyj0NdU1Z/2tQuBB4Hh7mvnqGqLiCwGVuPU8D9ts/6jwJsislVVTxCRa4HZ7gfHDuDkA8S4SkR+Bbztrt+Ik4z34czA1XoQ9q1vBK6O3qv269wB/F2cTp+1fN3O93+Ah8SZxLsZ55zLbBG5EnhORBLd9X6F8+E2V5yJzgW4xX3uLhEZ4S57F1gKLKPjv9UcnHMMq9y45+/vfTHesO6cxoQ4t9z0qqq+6HUsJjJYqccYY6KMHfEbY0yUsSN+Y4yJMpb4jTEmyljiN8aYKGOJ3xhjoowlfmOMiTL/Hww1encjeGLOAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"-ebCqoFc2vwN"},"id":"-ebCqoFc2vwN"},{"cell_type":"markdown","source":["Now let's test the model we obtained showing the generator output for training image."],"metadata":{"id":"nqt3UjGA2zmS"},"id":"nqt3UjGA2zmS"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator_lpips.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"TZve2kdp_2lF"},"id":"TZve2kdp_2lF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Input image:')\n","plot_image(input_img[0])"],"metadata":{"id":"YZcaz0q63G9y"},"id":"YZcaz0q63G9y","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The test errors for this new model are:"],"metadata":{"id":"dv1RLKHfNMZp"},"id":"dv1RLKHfNMZp"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator_lpips, discriminator_lpips, generator_loss, discriminator_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERo3uzBlNQNm","executionInfo":{"status":"ok","timestamp":1674571506343,"user_tz":-60,"elapsed":6923,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}},"outputId":"d8c81793-d974-4d9a-90a3-fa6c12f24f15"},"id":"ERo3uzBlNQNm","execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["(33.18053932189942, 0.3678408771753311)\n"]}]},{"cell_type":"markdown","source":["##7. Using WGAN as training mechanism\n"],"metadata":{"id":"e6a3FqSiwTN2"},"id":"e6a3FqSiwTN2"},{"cell_type":"markdown","source":["### Changes introduced"],"metadata":{"id":"RjFdHdGb3bph"},"id":"RjFdHdGb3bph"},{"cell_type":"markdown","source":["Firstly, we have developed a new Discriminator class, called <code>DiscriminatorWGAN</code>, corresponding to the WGAN architecture."],"metadata":{"id":"SpoyL0J3NgLp"},"id":"SpoyL0J3NgLp"},{"cell_type":"code","source":["class DiscriminatorWGAN(nn.Module):\n","    def __init__(self, in_channels, kernel_size, stride=1):\n","        super().__init__()\n","        \n","        self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2_bn = nn.BatchNorm2d(128)\n","        self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer3_bn = nn.BatchNorm2d(256)\n","        self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=kernel_size, padding=1) # stride = 1\n","        self.layer4_bn = nn.BatchNorm2d(512)\n","        self.layer5 = nn.Conv2d(512, out_channels=1, kernel_size=kernel_size, padding=1)\n","        \n","        # Weight initialization\n","        torch.nn.init.normal_(self.layer1.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer2.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer3.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer4.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer5.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer2_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer2_bn.bias.data, 0.0)\n","        torch.nn.init.normal_(self.layer3_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer3_bn.bias.data, 0.0)\n","        torch.nn.init.normal_(self.layer4_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer4_bn.bias.data, 0.0)\n","\n","    def forward(self, x, verbose=False):\n","        d = F.leaky_relu(self.layer1(x), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer2_bn(self.layer2(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer3_bn(self.layer3(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer4_bn(self.layer4(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = self.layer5(d)\n","        if verbose:\n","          print(d.shape)\n","\n","        return torch.sigmoid(d)"],"metadata":{"id":"bz5MHEwGwb3e","executionInfo":{"status":"ok","timestamp":1674571125622,"user_tz":-60,"elapsed":279,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"bz5MHEwGwb3e","execution_count":122,"outputs":[]},{"cell_type":"markdown","source":["The WGAN architecture requires also implementing a gradient penalty function:"],"metadata":{"id":"m3kOSm7hNu0_"},"id":"m3kOSm7hNu0_"},{"cell_type":"code","source":["def compute_gp(discriminator, real_data, fake_data):\n","  \n","  batch_size = real_data.size(0)\n","  \n","  # Sample Epsilon from uniform distribution\n","  eps = torch.rand(batch_size, 1, 1, 1).to(real_data.device)\n","  eps = eps.expand_as(real_data)\n","        \n","  # Interpolation between real data and fake data.\n","  interpolation = eps * real_data + (1 - eps) * fake_data\n","        \n","  # get logits for interpolated images\n","  interp_logits = discriminator(interpolation)\n","  grad_outputs = torch.ones_like(interp_logits)\n","        \n","  # Compute Gradients\n","  gradients = autograd.grad(\n","    outputs=interp_logits,\n","    inputs=interpolation,\n","    grad_outputs=grad_outputs,\n","    create_graph=True,\n","    retain_graph=True,\n","  )[0]\n","        \n","  # Compute and return Gradient Norm\n","  gradients = gradients.view(batch_size, -1)\n","  grad_norm = gradients.norm(2, 1)\n","  return torch.mean((grad_norm - 1) ** 2)"],"metadata":{"id":"UenzlqaZzhsx","executionInfo":{"status":"ok","timestamp":1674571128467,"user_tz":-60,"elapsed":390,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"UenzlqaZzhsx","execution_count":123,"outputs":[]},{"cell_type":"markdown","source":["And we will also use another discriminator loss function:"],"metadata":{"id":"5ZYsyWgUNz9l"},"id":"5ZYsyWgUNz9l"},{"cell_type":"code","source":["def discriminator_loss_wgan(output, label, gradient_penalty, c_lambda=10):\n","  return torch.mean(output - label  + gradient_penalty * c_lambda)"],"metadata":{"id":"7OityPTn7Aej","executionInfo":{"status":"ok","timestamp":1674571132317,"user_tz":-60,"elapsed":3,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"7OityPTn7Aej","execution_count":124,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"5m8H-QdO3hf0"},"id":"5m8H-QdO3hf0"},{"cell_type":"code","source":["generator_wgan = Generator(3)\n","generator_wgan.to(device)\n","generator_optimizer_wgan = op.Adam(generator_wgan.parameters(), lr=0.0002, weight_decay=0.5)\n","\n","# NEW: the discriminator class has changed\n","discriminator_wgan = DiscriminatorWGAN(in_channels=6, kernel_size=4)\n","discriminator_wgan.to(device)\n","discriminator_optimizer_wgan = op.Adam(discriminator_wgan.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"w1yjYZ708EIC","executionInfo":{"status":"ok","timestamp":1674571252075,"user_tz":-60,"elapsed":1619,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"w1yjYZ708EIC","execution_count":126,"outputs":[]},{"cell_type":"code","source":["generator_wgan_path = \"drive/MyDrive/models/generatorModelImageColoringWGAN.pth\"\n","discriminator_wgan_path = \"drive/MyDrive/models/discriminatorModelImageColoringWGAN.pth\"\n","\n","generator_wgan_losses = list()\n","discriminator_wgan_losses = list()\n","\n","num_epochs = 10\n","cnt = 1\n","\n","for epoch in range(1, num_epochs + 1):\n","  for (input_img, tg_img) in dataloader:\n","\n","    discriminator_optimizer_wgan.zero_grad()\n","\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = generator_wgan(input_img)\n","\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","    discriminator_fake = discriminator_wgan(disc_inp_fake.detach())\n","    \n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","    discriminator_real = discriminator_wgan(disc_inp_real)\n","\n","    gradient_penalty = compute_gp(discriminator_wgan, disc_inp_real, disc_inp_fake)\n","\n","    # NEW: the discriminator loss has changed\n","    discriminator_fake_loss = discriminator_loss_wgan(discriminator_fake, fake_target, gradient_penalty)\n","    discriminator_real_loss = discriminator_loss_wgan(discriminator_real,  real_target, gradient_penalty)\n","    \n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    discriminator_wgan_losses.append(discriminator_total_loss.item())\n","\n","    discriminator_total_loss.backward(retain_graph=True)\n","    discriminator_optimizer_wgan.step()\n","\n","    generator_optimizer_wgan.zero_grad()\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = discriminator_wgan(fake_gen)\n","    \n","    gen_loss = generator_loss(generated_image, tg_img, discriminator_prediction, real_target)\n","\n","    generator_wgan_losses.append(gen_loss.item())\n","\n","    gen_loss.backward(retain_graph=True)\n","    generator_optimizer_wgan.step()\n","\n","    if cnt % 10 == 0:\n","      print(\"Number of processed batches (batch_size = 1): \", cnt)\n","      save_model(generator_wgan, generator_wgan_path)\n","      save_model(discriminator_wgan, discriminator_wgan_path)\n","    \n","    cnt = cnt + 1"],"metadata":{"id":"Q4lpXy0M7_tQ"},"id":"Q4lpXy0M7_tQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plots for the generator and discriminator loss are the following:"],"metadata":{"id":"OGYYbSwjOns1"},"id":"OGYYbSwjOns1"},{"cell_type":"code","source":["plot_gen_loss(generator_wgan_losses)\n","plot_disc_loss(discriminator_wgan_losses)"],"metadata":{"id":"8Y5sykuSOtCZ"},"id":"8Y5sykuSOtCZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"13SuSAqT3xUU"},"id":"13SuSAqT3xUU"},{"cell_type":"markdown","source":["Now let's test the model we obtained showing the generator output for training image."],"metadata":{"id":"W4uMshN-O-iu"},"id":"W4uMshN-O-iu"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator_wgan.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"tIjh_PSP30cm"},"id":"tIjh_PSP30cm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Input image:')\n","plot_image(input_img[0])"],"metadata":{"id":"K6KeyJa8Q4xc"},"execution_count":null,"outputs":[],"id":"K6KeyJa8Q4xc"},{"cell_type":"markdown","source":["The test errors for this new model are:"],"metadata":{"id":"4lvSsaxDPCmx"},"id":"4lvSsaxDPCmx"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator_wgan, discriminator_wgan, generator_loss, discriminator_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5pXnx4-MPDF-","executionInfo":{"status":"ok","timestamp":1674571497239,"user_tz":-60,"elapsed":8358,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}},"outputId":"ac73ba26-5438-435b-a431-a8f65c5355f2"},"id":"5pXnx4-MPDF-","execution_count":132,"outputs":[{"output_type":"stream","name":"stdout","text":["(30.79624843597412, 1.2965497732162476)\n"]}]},{"cell_type":"markdown","source":["##7. Using WGAN and additional perceptual losses\n"],"metadata":{"id":"1BedRzhXRGvl"},"id":"1BedRzhXRGvl"},{"cell_type":"markdown","source":["###Training\n","\n","On this occasion, not only will we be training our former WGAN network but we will also introduce our <code>generator_loss_lpips</code> previously created."],"metadata":{"id":"6SbnOUvKS0Hb"},"id":"6SbnOUvKS0Hb"},{"cell_type":"code","source":["generator_all = Generator(3)\n","generator_all.to(device)\n","generator_optimizer_all = op.Adam(generator_all.parameters(), lr=0.0002, weight_decay=0.5)\n","\n","# NEW: the discriminator class has changed\n","discriminator_all = DiscriminatorWGAN(in_channels=6, kernel_size=4)\n","discriminator_all.to(device)\n","discriminator_optimizer_all = op.Adam(discriminator_all.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"0UvQKz8IPa98","executionInfo":{"status":"ok","timestamp":1674571565498,"user_tz":-60,"elapsed":1823,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"0UvQKz8IPa98","execution_count":134,"outputs":[]},{"cell_type":"code","source":["generator_all_path = \"drive/MyDrive/models/generatorModelImageColoringWGANandLpips.pth\"\n","discriminator_all_path = \"drive/MyDrive/models/discriminatorModelImageColoringWGANandLpips.pth\"\n","\n","generator_all_losses = list()\n","discriminator_all_losses = list()\n","\n","num_epochs = 10\n","cnt = 1\n","\n","for epoch in range(1, num_epochs + 1):\n","  for (input_img, tg_img) in dataloader:\n","\n","    discriminator_optimizer_all.zero_grad()\n","\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = generator_all(input_img)\n","\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","    discriminator_fake = discriminator_all(disc_inp_fake.detach())\n","    \n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","    discriminator_real = discriminator_all(disc_inp_real)\n","\n","    gradient_penalty = compute_gp(discriminator_all, disc_inp_real, disc_inp_fake)\n","\n","    discriminator_fake_loss = discriminator_loss_wgan(discriminator_fake, fake_target, gradient_penalty)\n","    discriminator_real_loss = discriminator_loss_wgan(discriminator_real,  real_target, gradient_penalty)\n","\n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    discriminator_all_losses.append(discriminator_total_loss.item())\n","\n","    discriminator_total_loss.backward(retain_graph=True)\n","    discriminator_optimizer_all.step()\n","\n","    generator_optimizer_all.zero_grad()\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = discriminator_all(fake_gen)\n","\n","    gen_loss = generator_loss_lpips(generated_image, tg_img, discriminator_prediction, real_target)                              \n","\n","    generator_all_losses.append(gen_loss.item())\n","\n","    gen_loss.backward(retain_graph=True)\n","    generator_optimizer_all.step()\n","\n","    if cnt % 10 == 0:\n","      print(\"Number of processed batches (batch_size = 1): \", cnt)\n","      save_model(generator_all, generator_all_path)\n","      save_model(discriminator_all, discriminator_all_path)\n","    \n","    cnt = cnt + 1"],"metadata":{"id":"pPuE_7xHRTLM"},"id":"pPuE_7xHRTLM","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plots for the generator and discriminator loss are the following:"],"metadata":{"id":"ZMmnS2bcRk9U"},"id":"ZMmnS2bcRk9U"},{"cell_type":"code","source":["plot_gen_loss(generator_all_losses)\n","plot_disc_loss(discriminator_all_losses)"],"metadata":{"id":"LKZPcISMRk9V"},"execution_count":null,"outputs":[],"id":"LKZPcISMRk9V"},{"cell_type":"markdown","source":["###Test"],"metadata":{"id":"6-gF7PVqStxI"},"id":"6-gF7PVqStxI"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator_wgan.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"eRj1T_qnSUFE"},"execution_count":null,"outputs":[],"id":"eRj1T_qnSUFE"},{"cell_type":"markdown","source":["The test errors for this new model are:"],"metadata":{"id":"M_WBZIANRGUt"},"id":"M_WBZIANRGUt"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator_all, discriminator_all, generator_loss, discriminator_loss))"],"metadata":{"id":"POGznJprRGUv"},"execution_count":null,"outputs":[],"id":"POGznJprRGUv"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}