{"cells":[{"cell_type":"markdown","source":["#Image Recolouring Project\n","\n","Project developed by Alejandro Cano Caldero and Jesús Moncada Ramírez for <i>Neural Networks and Deep Learning</i>, University of Padova, 2022-23.\n"],"metadata":{"id":"XRlCDiXymmvi"},"id":"XRlCDiXymmvi"},{"cell_type":"code","source":["!pip install lpips"],"metadata":{"id":"ripgenX5I17Y"},"id":"ripgenX5I17Y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torchmetrics[image]"],"metadata":{"id":"5zuVTRwfJEPr"},"id":"5zuVTRwfJEPr","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"33804e43","metadata":{"id":"33804e43","executionInfo":{"status":"ok","timestamp":1674722393007,"user_tz":-60,"elapsed":7,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"outputs":[],"source":["# Import necessary libraries for our project.\n","import numpy as np\n","\n","import torch\n","\n","import matplotlib.pyplot as plt\n","\n","from torchvision import transforms, datasets\n","from torchvision.transforms import transforms\n","\n","from torch.utils.data import DataLoader, Dataset  \n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as op\n","\n","from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n","\n","from torch import autograd"]},{"cell_type":"code","execution_count":4,"id":"b6b86a5a","metadata":{"id":"b6b86a5a","executionInfo":{"status":"ok","timestamp":1674722366192,"user_tz":-60,"elapsed":9,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"outputs":[],"source":["# Define the execution device.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# Mount in Google Drive. \n","# Only execute if running in Google Colab and the dataset is on Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OyX2-8kfpi1z"},"id":"OyX2-8kfpi1z","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##0. Useful functions##"],"metadata":{"id":"-qh1mAO1lQpG"},"id":"-qh1mAO1lQpG"},{"cell_type":"markdown","source":["In this section, we include some useful functions that will be used in this notebook."],"metadata":{"id":"_8T0XTrqlUAt"},"id":"_8T0XTrqlUAt"},{"cell_type":"code","source":["# Plots tensor image, its values must be in [0,1] or [0,255]\n","def plot_image(img, where=None):\n","  if where is None:\n","    plt.imshow(img.permute(1, 2, 0))\n","  else:\n","    where.imshow(img.permute(1, 2, 0))\n","\n","# Plots tensor image, its values must be in [-1,1]\n","def plot_image_minus1to1(image, where=None):\n","  if where is None:\n","    plot_image((image + 1) / 2)\n","  else:\n","    plot_image((image + 1) / 2, where)\n","\n","# Plots 4 tensor images provided by our different generatos\n","# Note the images pixel must take values in [-1, 1]\n","def plot_4_images_minus1to1(img1, img2, img3, img4):\n","  f, axarr = plt.subplots(2,2)\n","  axarr[0,0].axis('off')\n","  axarr[0,1].axis('off')\n","  axarr[1,0].axis('off')\n","  axarr[1,1].axis('off')\n","  plot_image_minus1to1(img1, axarr[0,0])\n","  plot_image_minus1to1(img2, axarr[0,1])\n","  plot_image_minus1to1(img3, axarr[1,0])\n","  plot_image_minus1to1(img4, axarr[1,1])\n","  axarr[0,0].title.set_text('Basic model')\n","  axarr[0,1].title.set_text('Lpips model')\n","  axarr[1,0].title.set_text('WGAN model')\n","  axarr[1,1].title.set_text('WGAN + Lpips model')\n","  #plt.subplots_adjust(wspace=.05, hspace=.05)\n","  plt.show()"],"metadata":{"id":"gJDogFiklgHr","executionInfo":{"status":"ok","timestamp":1674738233417,"user_tz":-60,"elapsed":294,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"gJDogFiklgHr","execution_count":167,"outputs":[]},{"cell_type":"code","source":["# Saves a model in a path\n","def save_model(model, path, verbose=False):\n","  torch.save(model.state_dict(), path)\n","  if verbose:\n","    print('Saved model '+path)\n","\n","# Loads a model from a path\n","# Note that model must match with the saved data in path\n","def load_model(model, path, verbose=False):\n","  if verbose:\n","    print('Loading model '+path)\n","  model.load_state_dict(torch.load(path))\n","  model.eval()\n","  return model"],"metadata":{"id":"Dq_VIrbym9Tg","executionInfo":{"status":"ok","timestamp":1674733808134,"user_tz":-60,"elapsed":265,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"Dq_VIrbym9Tg","execution_count":101,"outputs":[]},{"cell_type":"code","source":["# Plots the discriminator loss during training\n","def plot_disc_loss(d_losses):\n","\n","  fig = plt.figure()\n","  ax = plt.axes()\n","\n","  plt.title(\"Discriminator loss\")\n","  plt.xlabel(\"Number of batches processed\")\n","  plt.ylabel(\"Discriminator loss\");\n","\n","  x = list(range(1, len(d_losses)+1))\n","\n","  ax.plot(x, d_losses)\n","\n","# Plots the generator loss during training\n","def plot_gen_loss(g_losses):\n","\n","  fig = plt.figure()\n","  ax = plt.axes()\n","\n","  plt.title(\"Generator loss\")\n","  plt.xlabel(\"Number of batches processed\")\n","  plt.ylabel(\"Generator loss\");\n","\n","  x = list(range(1, len(g_losses)+1))\n","\n","  ax.plot(x, g_losses)"],"metadata":{"id":"4foyeivDnjQ_","executionInfo":{"status":"ok","timestamp":1674728876584,"user_tz":-60,"elapsed":562,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"4foyeivDnjQ_","execution_count":68,"outputs":[]},{"cell_type":"markdown","source":["##1. Dataset"],"metadata":{"id":"yo4NF3Wam8QA"},"id":"yo4NF3Wam8QA"},{"cell_type":"markdown","source":["The dataset we have used is [ImageNette](https://github.com/fastai/imagenette), a reduced version of ImageNet, specifically the version of the full-size images."],"metadata":{"id":"0YIG1eDIkQ82"},"id":"0YIG1eDIkQ82"},{"cell_type":"markdown","source":["###Classes###"],"metadata":{"id":"j8YXHYk2iVBY"},"id":"j8YXHYk2iVBY"},{"cell_type":"markdown","source":["<code>ImageDataset</code> is a default dataset class for images . To create an instance of this class you have to specify the route of the image folder (<code>image_path</code>) and a list of transforms (<code>transforms</code>). The constructor parameters <code>first</code> and <code>last</code> allow us using a subset of the full dataset specified in the path (as in our case). Anyway, the whole dataset is loaded, but only these elements will be available.\n","\n","Basically it is an adapted version of <code>torchvision.datasets.ImageFolder</code>. Note that only returns one image when accessing to an element in <code>\\_\\_getitem\\_\\_</code>."],"metadata":{"id":"RdFNSV_H7o5K"},"id":"RdFNSV_H7o5K"},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","  def __init__(self, image_path, transform=None, first=0, last=499):\n","    super(ImageDataset, self).__init__()\n","    self.data = datasets.ImageFolder(image_path,  transform)\n","    self.first = first\n","    self.last = last\n","\n","  def __getitem__(self, idx):\n","    x, y = self.data[self.first + idx]\n","    return x\n","\n","  def __len__(self):\n","    return self.last-self.first + 1"],"metadata":{"id":"aGiWrxuRooTz","executionInfo":{"status":"ok","timestamp":1674722398691,"user_tz":-60,"elapsed":308,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"aGiWrxuRooTz","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["<code>TwoImagesDataset</code> is a dataset class combining two images datasets. We will use this class to create a dataset with tuples containing two versions of the same image, the first element will be the grayscale version (input image) and the second element will be the colored version (target image)."],"metadata":{"id":"W_eYsVQa8caz"},"id":"W_eYsVQa8caz"},{"cell_type":"code","source":["class TwoImagesDataset(Dataset):\n","  def __init__(self, dataset1, dataset2):\n","    super(TwoImagesDataset, self).__init__()\n","    self.dataset1 = dataset1\n","    self.dataset2 = dataset2\n","    if len(self.dataset1) != len(self.dataset2):\n","      print('ERROR, len of dataset1 is different from len of dataset2')\n","\n","  def __getitem__(self, idx):\n","    return self.dataset1[idx], self.dataset2[idx]\n","\n","  def __len__(self):\n","    return len(self.dataset1)"],"metadata":{"id":"UEXGx53VUtuq","executionInfo":{"status":"ok","timestamp":1674722400732,"user_tz":-60,"elapsed":2,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"UEXGx53VUtuq","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["###Required transforms and image preprocessing###"],"metadata":{"id":"XlpJ1gpXia54"},"id":"XlpJ1gpXia54"},{"cell_type":"markdown","source":["After defining the data structures, we will define some transforms for the images in our dataset. The transforms specified here are required by the structures of our generator/discriminator and by the problem itself. The transforms are the following:\n","* <code>ToTensor</code>: to convert the images into tensors.\n","* <code>Normalize</code>: to convert every pixel value from <code>[0,255]</code> to <code>[0,1]</code> (needed for discriminator).\n","* <code>Resize</code>: to resize all the images to <code>256 x 256</code>.\n","* <code>Grayscale</code>: to convert the original version of each image to the grayscale version, the input of our generator."],"metadata":{"id":"8UHn94PggtH5"},"id":"8UHn94PggtH5"},{"cell_type":"markdown","source":["As it was commented in the papers we studied, it is convenient to apply some image preprocessing to the images in our dataset, in order to remove some noise and improve the quality. The image preprocessing techniques we will use are:\n","*   **Gaussian blur** (<code>GaussianBlur</code>) in order to remove any Gaussian noise that could be present in our images. The kernel size has been set to 5 and standard deviation to <code>(0.7, 0.7)</code>.\n","*   **Histogram equalization** (<code>RandomEqualize</code>) in order to increase the contrast of images (especially those in grayscale, as we discovered that sometimes they looked extremely dark). This will facilitate the learning process."],"metadata":{"id":"L4HKSgQ6ieSe"},"id":"L4HKSgQ6ieSe"},{"cell_type":"code","source":["colored_transform_list = (\n","    [\n","     transforms.RandomEqualize(1),\n","     transforms.ToTensor(), # to tensor\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # normalization [0, 255] --> [-1, 1] (to the tanh activation function)\n","     transforms.Resize((256, 256)) # resize to (256, 256)\n","     transforms.GaussianBlur(kernel_size = 5, sigma=(0.7, 0.7))\n","     ])\n","\n","grayscale_transform_list = (\n","    [\n","     transforms.RandomEqualize(1),\n","     transforms.ToTensor(), # to tensor\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # normalization [0, 255] --> [-1, 1] (to the tanh activation function)\n","     transforms.Resize((256, 256)), # resize to (256, 256)\n","     transforms.Grayscale(3) # to grayscale\n","     transforms.GaussianBlur(kernel_size = 5, sigma=(0.7, 0.7))\n","     ])"],"metadata":{"id":"JZEUFs2Bgro8","executionInfo":{"status":"ok","timestamp":1674734033949,"user_tz":-60,"elapsed":617,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"JZEUFs2Bgro8","execution_count":109,"outputs":[]},{"cell_type":"markdown","source":["###Instances###"],"metadata":{"id":"ghBheR2Iks8p"},"id":"ghBheR2Iks8p"},{"cell_type":"markdown","source":["Now we have all the transforms defined, we can create the instances for our dataset and data loader. As you can observe, we will create a dataset containing the colored version of our images, and a dataset containing the grayscale version. By last we will create the dataset class containing both kinds of images (<code>code</code>), that will be used for training. In our case, it will be a toy dataset with 500 samples.\n","\n","We will also create a test dataset with 150 samples (30% of the training dataset), <code>dataset_test</code>."],"metadata":{"id":"bixeLxG4-3A4"},"id":"bixeLxG4-3A4"},{"cell_type":"code","source":["colored_transform = transforms.Compose(colored_transform_list)\n","grayscale_transform = transforms.Compose(grayscale_transform_list)"],"metadata":{"id":"2QXqANWkkwyn","executionInfo":{"status":"ok","timestamp":1674734039749,"user_tz":-60,"elapsed":392,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"2QXqANWkkwyn","execution_count":110,"outputs":[]},{"cell_type":"code","source":["# If locating the dataset in Google Drive\n","img_path_train = 'drive/MyDrive/imagenette2/train'\n","img_path_test = 'drive/MyDrive/imagenette2/val'\n","\n","# TRAINING SET 500 samples\n","colored_data = ImageDataset(img_path_train, colored_transform, first=0, last=499)\n","grayscale_data = ImageDataset(img_path_train, grayscale_transform, first=0, last=499)\n","dataset = TwoImagesDataset(grayscale_data, colored_data)\n","\n","# TEST SET 150 samples (30% of training samples)\n","colored_data_test = ImageDataset(img_path_test, colored_transform, first=0, last=149)\n","grayscale_data_test = ImageDataset(img_path_test, grayscale_transform, first=0, last=149)\n","dataset_test = TwoImagesDataset(colored_data_test, grayscale_data_test)"],"metadata":{"id":"yIwpJ3fipDXP","executionInfo":{"status":"ok","timestamp":1674734042368,"user_tz":-60,"elapsed":801,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"yIwpJ3fipDXP","execution_count":111,"outputs":[]},{"cell_type":"code","source":["print(\"Number of images in our training dataset is: {0}\".format(len(dataset)))\n","print(\"Number of images in our test dataset is: {0}\".format(len(dataset_test)))"],"metadata":{"id":"y_EFSLiPUob9"},"id":"y_EFSLiPUob9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's show the colored and the grayscale version of the same image."],"metadata":{"id":"oOYuE1Cv_ZtA"},"id":"oOYuE1Cv_ZtA"},{"cell_type":"code","execution_count":null,"id":"83b897bd","metadata":{"id":"83b897bd"},"outputs":[],"source":["grayscale_img, colored_img  = dataset[0]\n","\n","print('Plotting a grayscale image in the dataset:')\n","plot_image(grayscale_img)"]},{"cell_type":"code","source":["print('Plotting a colored image in the dataset:')\n","plot_image(colored_img)"],"metadata":{"id":"w2KWTWqXBlkX"},"id":"w2KWTWqXBlkX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need a <code>DataLoader</code> instance for our dataset in order to get the images as batches for the training phase. The <code>batch_size</code> will be 1 as said in the paper. We will also create a data loader for the test data."],"metadata":{"id":"RUEuLCNQnJxK"},"id":"RUEuLCNQnJxK"},{"cell_type":"code","source":["dataloader = DataLoader(dataset, batch_size=1)\n","\n","dataloader_test = DataLoader(dataset_test, batch_size=1)"],"metadata":{"id":"55cLAHs-Mlpe","executionInfo":{"status":"ok","timestamp":1674722550219,"user_tz":-60,"elapsed":263,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"55cLAHs-Mlpe","execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["##2. Discriminator"],"metadata":{"id":"b95tH6-FB7nE"},"id":"b95tH6-FB7nE"},{"cell_type":"markdown","source":["The discriminator used (called PatchGAN) takes as input an image of <code>(256x256)</code> and returns a tensor of <code>(30x30)</code>. The discriminator class is the following:"],"metadata":{"id":"6NSU-t1rCKlD"},"id":"6NSU-t1rCKlD"},{"cell_type":"code","execution_count":57,"id":"e32d0b1e","metadata":{"id":"e32d0b1e","executionInfo":{"status":"ok","timestamp":1674728593409,"user_tz":-60,"elapsed":284,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels, kernel_size, stride=1):\n","        super().__init__()\n","        \n","        self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2_bn = nn.BatchNorm2d(128)\n","        self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer3_bn = nn.BatchNorm2d(256)\n","        self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=kernel_size, padding=1) # stride = 1\n","        self.layer4_bn = nn.BatchNorm2d(512)\n","        self.layer5 = nn.Conv2d(512, out_channels=1, kernel_size=kernel_size, padding=1)\n","        \n","        # Weight initialization\n","        torch.nn.init.normal_(self.layer1.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer2.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer3.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer4.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer5.weight.data, mean=0.0, std=0.02)\n","        torch.nn.init.normal_(self.layer2_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer2_bn.bias.data, 0.0)\n","        torch.nn.init.normal_(self.layer3_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer3_bn.bias.data, 0.0)\n","        torch.nn.init.normal_(self.layer4_bn.weight.data, mean=1.0, std=0.02)\n","        torch.nn.init.constant_(self.layer4_bn.bias.data, 0.0)\n","\n","    def forward(self, x, verbose=False):\n","        d = F.leaky_relu(self.layer1(x), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer2_bn(self.layer2(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer3_bn(self.layer3(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = F.leaky_relu(self.layer4_bn(self.layer4(d)), 0.2)\n","        if verbose:\n","          print(d.shape)\n","        d = self.layer5(d)\n","        if verbose:\n","          print(d.shape)\n","\n","        return torch.sigmoid(d)"]},{"cell_type":"markdown","source":["Let's create an object for the discriminator and its Adam optimizer."],"metadata":{"id":"8n-zLnNECNXw"},"id":"8n-zLnNECNXw"},{"cell_type":"code","source":["discriminator = Discriminator(in_channels=6, kernel_size=4)\n","discriminator.to(device)\n","\n","discriminator_optimizer = op.Adam(discriminator.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"2jZyxqWrCQCp","executionInfo":{"status":"ok","timestamp":1674728595609,"user_tz":-60,"elapsed":286,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"2jZyxqWrCQCp","execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["And let's execute the discriminator forward pass on a pair of images concatenated from the dataset to check if it works."],"metadata":{"id":"SEyuFfkFCXSt"},"id":"SEyuFfkFCXSt"},{"cell_type":"code","source":["input_img, target_img = dataset[0]\n","\n","input_img = input_img.unsqueeze(0)\n","target_img = target_img.unsqueeze(0)\n","\n","disc_input = torch.cat((input_img, target_img), dim=1) # discriminator input\n","\n","print(\"Shape of discriminator input: {0}\".format(disc_input.shape))\n","\n","out = discriminator.forward(disc_input) # forward pass discriminator\n","\n","print(\"Shape of discriminator output: {0}\".format(out.shape))"],"metadata":{"id":"VQUWlIcaQC5W"},"id":"VQUWlIcaQC5W","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3. Generator"],"metadata":{"id":"CEAnLosJ7B2Y"},"id":"CEAnLosJ7B2Y"},{"cell_type":"markdown","source":["The generator used has an autoencoder style, with both encoder and decoder networks. The input and the output are images. It uses a UNET Generator, with skip connections. The generator class is the following:"],"metadata":{"id":"OI8Jt-zg7Ga5"},"id":"OI8Jt-zg7Ga5"},{"cell_type":"code","source":["class Generator(nn.Module):\n","  def __init__(self, in_channels, stride=1):\n","    super().__init__()\n","\n","    # ----------------------------- ENCODER ----------------------------\n","    #            ENCODER MODEL: C64-C128-C256-C512-C512-C512-C512\n","    self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=4, stride=2, padding=1)\n","    self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=4, stride=2, padding=1)\n","    self.layer2_bn = nn.BatchNorm2d(128)\n","    self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=4, stride=2, padding=1)\n","    self.layer3_bn = nn.BatchNorm2d(256)\n","    self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer4_bn = nn.BatchNorm2d(512)\n","    self.layer5 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer5_bn = nn.BatchNorm2d(512)\n","    self.layer6 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer6_bn = nn.BatchNorm2d(512)\n","    self.layer7 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer7_bn = nn.BatchNorm2d(512)\n","\n","    # ----------------------------- BOTTLENECK ----------------------------\n","    self.bottleneck_layer = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","\n","    # ----------------------------- DECODER ----------------------------\n","    #           DECODER MODEL: CD512-CD512-CD512-CD512-CD256-CD128-CD64\n","    self.layer8 = nn.ConvTranspose2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer8_bn = nn.BatchNorm2d(512)\n","    self.layer8_dpout = nn.Dropout()\n","    self.layer9 = nn.ConvTranspose2d(1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer9_bn = nn.BatchNorm2d(512)\n","    self.layer9_dpout = nn.Dropout()\n","    self.layer10 = nn.ConvTranspose2d(1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer10_bn = nn.BatchNorm2d(512)\n","    self.layer10_dpout = nn.Dropout()\n","    self.layer11 = nn.ConvTranspose2d(1024, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer11_bn = nn.BatchNorm2d(512)\n","    self.layer12 = nn.ConvTranspose2d(1024, out_channels=256, kernel_size=4, stride=2, padding=1)\n","    self.layer12_bn = nn.BatchNorm2d(256)\n","    self.layer13 = nn.ConvTranspose2d(512, out_channels=128, kernel_size=4, stride=2, padding=1)\n","    self.layer13_bn = nn.BatchNorm2d(128)\n","    self.layer14 = nn.ConvTranspose2d(256, out_channels=64, kernel_size=4, stride=2, padding=1)\n","    self.layer14_bn = nn.BatchNorm2d(64)\n","\n","    # ----------------------------- OUTPUT ----------------------------\n","    self.layer15 = nn.ConvTranspose2d(128, out_channels=3, kernel_size=4, stride=2, padding=1)\n","\n","    # Weight initialization\n","    torch.nn.init.normal_(self.layer1.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer2.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer3.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer4.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer5.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer6.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer7.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.bottleneck_layer.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer8.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer9.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer10.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer11.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer12.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer13.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer14.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer15.weight.data, mean=0.0, std=0.02)\n","    torch.nn.init.normal_(self.layer2_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer2_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer3_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer3_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer4_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer4_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer5_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer5_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer6_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer6_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer7_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer7_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer8_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer8_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer9_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer9_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer10_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer10_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer11_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer11_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer12_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer12_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer13_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer13_bn.bias.data, 0.0)\n","    torch.nn.init.normal_(self.layer14_bn.weight.data, mean=1.0, std=0.02)\n","    torch.nn.init.constant_(self.layer14_bn.bias.data, 0.0)\n","\n","  def forward(self, x, verbose=False):\n","\n","    # ----------------------------- ENCODER ----------------------------\n","    e1 = F.leaky_relu(self.layer1(x), 0.2)\n","    if verbose:\n","      print(e1.shape)\n","    e2 = F.leaky_relu(self.layer2_bn(self.layer2(e1)), 0.2)\n","    if verbose:\n","      print(e2.shape)\n","    e3 = F.leaky_relu(self.layer3_bn(self.layer3(e2)), 0.2)\n","    if verbose:\n","      print(e3.shape)\n","    e4 = F.leaky_relu(self.layer4_bn(self.layer4(e3)), 0.2)\n","    if verbose:\n","      print(e4.shape)\n","    e5 = F.leaky_relu(self.layer5_bn(self.layer5(e4)), 0.2)\n","    if verbose:\n","      print(e5.shape)\n","    e6 = F.leaky_relu(self.layer6_bn(self.layer6(e5)), 0.2)\n","    if verbose:\n","      print(e6.shape)\n","    e7 = F.leaky_relu(self.layer7_bn(self.layer7(e6)), 0.2) \n","    if verbose:\n","      print(e7.shape)\n","\n","    # ----------------------------- BOTTLENECK ----------------------------\n","    b = F.relu(self.bottleneck_layer(e7))\n","    if verbose:\n","      print(b.shape)\n","\n","    # ----------------------------- DECODER ----------------------------\n","    d1 = F.relu(torch.cat((self.layer8_dpout(self.layer8_bn(self.layer8(b))), e7), 1))\n","    if verbose:\n","      print(d1.shape)\n","    d2 = F.relu(torch.cat((self.layer9_dpout(self.layer9_bn(self.layer9(d1))), e6), 1))\n","    if verbose:\n","      print(d2.shape)\n","    d3 = F.relu(torch.cat((self.layer10_dpout(self.layer10_bn(self.layer10(d2))), e5), 1))\n","    if verbose:\n","      print(d3.shape)\n","    d4 = F.relu(torch.cat(((self.layer11_bn(self.layer11(d3))), e4), 1))\n","    if verbose:\n","      print(d4.shape)\n","    d5 = F.relu(torch.cat(((self.layer12_bn(self.layer12(d4))), e3), 1))\n","    if verbose:\n","      print(d5.shape)\n","    d6 = F.relu(torch.cat(((self.layer13_bn(self.layer13(d5))), e2), 1))\n","    if verbose:\n","      print(d6.shape)\n","    d7 = F.relu(torch.cat(((self.layer14_bn(self.layer14(d6))), e1), 1))\n","    if verbose:\n","      print(d7.shape)\n","\n","    # ----------------------------- OUTPUT ----------------------------\n","    o = torch.tanh(self.layer15(d7))\n","\n","    return o"],"metadata":{"id":"g6SCuigGi7WC","executionInfo":{"status":"ok","timestamp":1674728625034,"user_tz":-60,"elapsed":367,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"g6SCuigGi7WC","execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["Let's create an object for the generator and its Adam optimizer."],"metadata":{"id":"3F_lufO5s6_o"},"id":"3F_lufO5s6_o"},{"cell_type":"code","source":["generator = Generator(3)\n","generator.to(device)\n","\n","generator_optimizer = op.Adam(generator.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"w90yleohsi_C","executionInfo":{"status":"ok","timestamp":1674728631156,"user_tz":-60,"elapsed":1279,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"w90yleohsi_C","execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["And let's execute the generator forward pass on some images from the dataset to check if it works."],"metadata":{"id":"3J25gYESxm9z"},"id":"3J25gYESxm9z"},{"cell_type":"code","source":["input_img, tg_img = dataset[0]\n","input = input_img.unsqueeze(0)\n","\n","print(\"Shape of generator input: {0}\".format(input.shape))\n","\n","out = generator.forward(input)\n","\n","print(\"Shape of generator output: {0} \".format(out.shape))\n","\n","out = out.detach()\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"OdfwyxIqszxO"},"id":"OdfwyxIqszxO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##4. Loss functions"],"metadata":{"id":"A5BzSbPh_ahG"},"id":"A5BzSbPh_ahG"},{"cell_type":"markdown","source":["For the loss function, we are going to use the **Binary Cross-Entropy** loss and the **L1** loss, as explained in the original paper."],"metadata":{"id":"UFgM1oGG-3gC"},"id":"UFgM1oGG-3gC"},{"cell_type":"code","source":["adversarial_loss = nn.BCELoss(weight=torch.tensor(0.5))\n","l1_loss = nn.L1Loss()"],"metadata":{"id":"e45LZ-VXN-R5","executionInfo":{"status":"ok","timestamp":1674728655993,"user_tz":-60,"elapsed":268,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"e45LZ-VXN-R5","execution_count":65,"outputs":[]},{"cell_type":"markdown","source":["Four parameters are fed to the <code>generator_loss</code> function:\n","\n","1. <code>generator_image</code>: Images produced by the generator.\n","2. <code>target_image</code>: Ground-truth pair image for the input fed to the generator.\n","3. <code>discriminator_predictions</code>: Output predictions from the discriminator, when fed with generator-produced images.\n","4. <code>real_target</code>: Ground-truth labels (1), as you would like the generator to produce real images by fooling the discriminator. The labels therefore would be one."],"metadata":{"id":"IjfuXc569pNv"},"id":"IjfuXc569pNv"},{"cell_type":"code","source":["def generator_loss(generator_image, target_image, discriminator_predictions, real_target):\n","  gen_loss = adversarial_loss(discriminator_predictions, real_target)\n","  l1_l = l1_loss(generator_image, target_image)\n","  result = gen_loss + (100 * l1_l)\n","\n","  return result"],"metadata":{"id":"FhDadAG5OKD1","executionInfo":{"status":"ok","timestamp":1674728675900,"user_tz":-60,"elapsed":279,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"FhDadAG5OKD1","execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["The <code>discriminator_loss</code> function has 2 arguments:\n","\n","1. <code>output</code>: Output of the discriminator for a pair of images as input.\n","2. <code>label</code>: Expected output for the discriminator, a tensor full of 0's (in case of fake images) or a tensor full of 1's (in case of true images)."],"metadata":{"id":"4kCeF_S5-nvG"},"id":"4kCeF_S5-nvG"},{"cell_type":"code","source":["def discriminator_loss(output, label):\n","  return adversarial_loss(output, label)"],"metadata":{"id":"lbRSIgnfOPDV","executionInfo":{"status":"ok","timestamp":1674728679059,"user_tz":-60,"elapsed":384,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"lbRSIgnfOPDV","execution_count":67,"outputs":[]},{"cell_type":"markdown","source":["##5. Training"],"metadata":{"id":"iQlLrv5DM5ug"},"id":"iQlLrv5DM5ug"},{"cell_type":"markdown","source":["The training phase for our model will be the following. Note <code>num_epochs</code> has been set to 1 in order to test everything, but should be a higher value. The training losses (from the generator and discriminator) obtained at each batch processed will be saved into two lists, in order to plot them later."],"metadata":{"id":"mh3GLO7ZvMWg"},"id":"mh3GLO7ZvMWg"},{"cell_type":"code","source":["# Info to save the models\n","GENERATOR_MODEL_PATH = \"drive/MyDrive/models/generatorModelImageColoring.pth\"\n","DISCRIMINATOR_MODEL_PATH = \"drive/MyDrive/models/discriminatorModelImageColoring.pth\"\n","\n","# Number of epochs\n","NUM_EPOCHS = 10 \n","\n","# Count\n","cnt = 1\n","\n","# List of generator losses\n","generator_losses = list()\n","# List of discriminator losses\n","discriminator_losses = list()\n","\n","for epoch in range(1, NUM_EPOCHS + 1):\n","\n","  for (input_img, tg_img) in dataloader:\n","\n","    discriminator_optimizer.zero_grad()\n","\n","    # Put input and target image into device.\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    # Ground truth labels real and fake\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    # Generator forward pass\n","    generated_image = generator(input_img)\n","\n","    # Train discriminator with fake/generated images\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","\n","    discriminator_fake = discriminator(disc_inp_fake.detach())\n","    discriminator_fake_loss = discriminator_loss(discriminator_fake, fake_target)\n","\n","    # Train discriminator with real images\n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","\n","    discriminator_real = discriminator(disc_inp_real)\n","    discriminator_real_loss = discriminator_loss(discriminator_real,  real_target)\n","\n","    # Average discriminator loss\n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    # Save discriminator loss into the list\n","    discriminator_losses.append(discriminator_total_loss.item())\n","\n","    # Compute gradients and run optimizer step\n","    discriminator_total_loss.backward()\n","    discriminator_optimizer.step()\n","\n","    # Train generator with real labels\n","    generator_optimizer.zero_grad()\n","\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = discriminator(fake_gen)\n","\n","    gen_loss = generator_loss(generated_image, tg_img, discriminator_prediction, real_target)                              \n","\n","    # Save generator loss into the list\n","    generator_losses.append(gen_loss.item())\n","\n","    # Compute gradients and run optimizer step\n","    gen_loss.backward()\n","    generator_optimizer.step()\n","\n","    # Notify state of training and save models\n","    if cnt % 10 == 0:\n","      print(\"CURRENT STATE: Epoch: {0}, Number of batches processed: {1}\".format(epoch, cnt))\n","      save_model(generator, GENERATOR_MODEL_PATH)\n","      save_model(discriminator, DISCRIMINATOR_MODEL_PATH)\n","\n","    cnt = cnt + 1"],"metadata":{"id":"tq6PHXr0L9zx"},"id":"tq6PHXr0L9zx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When the training is finished, let's plot the generator and discriminator training loss during the process."],"metadata":{"id":"TXFTsrwZIlJN"},"id":"TXFTsrwZIlJN"},{"cell_type":"code","source":["plot_disc_loss(discriminator_losses)\n","plot_gen_loss(generator_losses)"],"metadata":{"id":"qKUnY5zsxfJa"},"id":"qKUnY5zsxfJa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try to save and load the models form disc. Anyway, they have been saved each 10 iterations during training."],"metadata":{"id":"wEo6xMNuARDu"},"id":"wEo6xMNuARDu"},{"cell_type":"code","source":["save_model(generator, GENERATOR_MODEL_PATH, verbose=True)\n","save_model(discriminator, DISCRIMINATOR_MODEL_PATH, verbose=True)"],"metadata":{"id":"BQDjnYFq5fpu"},"id":"BQDjnYFq5fpu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = Generator(3)\n","load_model(generator, GENERATOR_MODEL_PATH)\n","\n","discriminator = Discriminator(in_channels=6, kernel_size=4)\n","load_model(discriminator, DISCRIMINATOR_MODEL_PATH, verbose=True)"],"metadata":{"id":"gi6POkY4AovX"},"id":"gi6POkY4AovX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##6. Test"],"metadata":{"id":"bARc6yNZuWqf"},"id":"bARc6yNZuWqf"},{"cell_type":"markdown","source":["We will execute our generator on the first image of the training dataset, to see if it's working properly."],"metadata":{"id":"aMY9iTYoKndD"},"id":"aMY9iTYoKndD"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"3PH9qqYy4ok_"},"id":"3PH9qqYy4ok_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Input image:')\n","plot_image(input_img[0])"],"metadata":{"id":"Ay3Zc6uB5BWZ"},"id":"Ay3Zc6uB5BWZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we implement a function to calculate the test error of our model, its arguments are:\n","- <code>data</code>: <code>DataLoader</code> object with the test dataset.\n","- <code>g</code>: generator instance.\n","- <code>d</code>: discriminator instance.\n","- <code>g_l</code>: generator loss function.\n","- <code>d_l</code>: discriminator loss function\n","It will return the generator test error and the discriminator test error in a tuple."],"metadata":{"id":"isJqpedNKyvR"},"id":"isJqpedNKyvR"},{"cell_type":"code","source":["def get_test_error(data, g, d, g_l, d_l):\n","  gen_loss_return = 0.0\n","  dis_loss_return = 0.0\n","  \n","  for (input_img, tg_img) in data:\n","\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = g(input_img)\n","\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","\n","    discriminator_fake = d(disc_inp_fake.detach())\n","    discriminator_fake_loss = d_l(discriminator_fake, fake_target)\n","\n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","\n","    discriminator_real = d(disc_inp_real)\n","    discriminator_real_loss = d_l(discriminator_real,  real_target)\n","\n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    dis_loss_return = dis_loss_return + discriminator_total_loss.item()\n","\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    \n","    discriminator_prediction = d(fake_gen)\n","    gen_loss = g_l(generated_image, tg_img, discriminator_prediction, real_target).item()                            \n","\n","    gen_loss_return = gen_loss_return + gen_loss\n","  \n","  return gen_loss_return / len(data), dis_loss_return / len(data)"],"metadata":{"id":"e7BKHuSTuX-w","executionInfo":{"status":"ok","timestamp":1674735504048,"user_tz":-60,"elapsed":458,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"e7BKHuSTuX-w","execution_count":149,"outputs":[]},{"cell_type":"markdown","source":["So the test errors for our model are:"],"metadata":{"id":"4CQxiU2iLVex"},"id":"4CQxiU2iLVex"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator, discriminator, generator_loss, discriminator_loss))"],"metadata":{"id":"VDI-n7KHvciW"},"id":"VDI-n7KHvciW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Using additional perceptual losses (<code>torchmetrics</code>)\n"],"metadata":{"id":"NNv8F8e1qQq6"},"id":"NNv8F8e1qQq6"},{"cell_type":"markdown","source":["### Changes introduced"],"metadata":{"id":"Yky3JHPEy3ti"},"id":"Yky3JHPEy3ti"},{"cell_type":"markdown","source":["The first improvement we have implemented, <code>LearnedPerceptualImagePatchSimilarity</code>, is used to judge the perceptual similarity between two images. This loss will be added to the generator loss, to penalize those generated images that are not similar to their input images."],"metadata":{"id":"fnN-0Gy_rB7e"},"id":"fnN-0Gy_rB7e"},{"cell_type":"code","source":["lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex')"],"metadata":{"id":"a4LRnxp4qZ_a"},"id":"a4LRnxp4qZ_a","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This way, we have to define a new generator loss adding the perceptual loss. We have decided to multiply its value by 50 to make it more important than the _adversarial_ loss but less than the _l1_ loss."],"metadata":{"id":"YcN-yIffrcl9"},"id":"YcN-yIffrcl9"},{"cell_type":"code","source":["def generator_loss_lpips(generator_image, target_image, discriminator_predictions, real_target):\n","  gen_loss = adversarial_loss(discriminator_predictions, real_target)\n","  l1_l = l1_loss(generator_image, target_image)\n","  perceptual_similarity = lpips(generator_image, target_image)\n","  result = gen_loss + (100 * l1_l) + (50 * perceptual_similarity)\n","\n","  return result"],"metadata":{"id":"UvBdirf4rAiR","executionInfo":{"status":"ok","timestamp":1674722486669,"user_tz":-60,"elapsed":4,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"UvBdirf4rAiR","execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"8NkWVuQ92Li0"},"id":"8NkWVuQ92Li0"},{"cell_type":"markdown","source":["Let's create objects for a new generator, a new discriminator, and their Adam optimizers."],"metadata":{"id":"fWKAFOsN2Q8x"},"id":"fWKAFOsN2Q8x"},{"cell_type":"code","source":["generator_lpips = Generator(3)\n","generator_lpips.to(device)\n","generator_optimizer_lpips = op.Adam(generator_lpips.parameters(), lr=0.0002, weight_decay=0.5)\n","\n","discriminator_lpips = Discriminator(in_channels=6, kernel_size=4)\n","discriminator_lpips.to(device)\n","discriminator_optimizer_lpips = op.Adam(discriminator_lpips.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"jgWwgdbMuH0U"},"id":"jgWwgdbMuH0U","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Therefore, the training phase would follow as:"],"metadata":{"id":"xHiejU-3rn7u"},"id":"xHiejU-3rn7u"},{"cell_type":"code","source":["GENERATOR_LPIPS_PATH = \"drive/MyDrive/models/generatorModelImageColoringLpips.pth\"\n","DISCRIMINATOR_LPIPS_PATH = \"drive/MyDrive/models/discriminatorModelImageColoringLpips.pth\"\n","\n","generator_lpips_losses = list()\n","discriminator_lpips_losses = list()\n","\n","NUM_EPOCHS = 10\n","cnt = 1\n","\n","for epoch in range(1, NUM_EPOCHS + 1):\n","  for (input_img, tg_img) in dataloader:\n","\n","    discriminator_optimizer_lpips.zero_grad()\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = generator_lpips(input_img)\n","\n","    disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","\n","    discriminator_fake = discriminator_lpips(disc_inp_fake.detach())\n","\n","    discriminator_fake_loss = discriminator_loss(discriminator_fake, fake_target)\n","\n","    disc_inp_real = torch.cat((input_img, target_img), 1)\n","\n","    discriminator_real = discriminator_lpips(disc_inp_real)\n","    discriminator_real_loss = discriminator_loss(discriminator_real,  real_target)\n","\n","    discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","    discriminator_lpips_losses.append(discriminator_total_loss.item())\n","\n","    discriminator_total_loss.backward()\n","    discriminator_optimizer_lpips.step()\n","\n","    generator_optimizer_lpips.zero_grad()\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = discriminator_lpips(fake_gen)\n","\n","    # NEW: The generator loss has changed!\n","    gen_loss = generator_loss_lpips(generated_image, tg_img, discriminator_prediction, real_target)                              \n","\n","    generator_lpips_losses.append(gen_loss.item())\n","\n","    gen_loss.backward()\n","    generator_optimizer_lpips.step()\n","\n","    if cnt % 10 == 0:\n","      print(\"CURRENT STATE: Epoch: {0}, Number of batches processed: {1}\".format(epoch, cnt))\n","      save_model(generator_lpips, GENERATOR_LPIPS_PATH)\n","      save_model(discriminator_lpips, DISCRIMINATOR_LPIPS_PATH)\n","    \n","    cnt = cnt + 1"],"metadata":{"id":"8z-8cpfjrxyn"},"id":"8z-8cpfjrxyn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plots for the generator and discriminator loss are the following:"],"metadata":{"id":"d8QpoIWUMign"},"id":"d8QpoIWUMign"},{"cell_type":"code","source":["plot_gen_loss(generator_lpips_losses)\n","plot_disc_loss(discriminator_lpips_losses)"],"metadata":{"id":"bwxZE15hMiB6"},"id":"bwxZE15hMiB6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"-ebCqoFc2vwN"},"id":"-ebCqoFc2vwN"},{"cell_type":"markdown","source":["Now let's test the model we obtained showing the generator output for the training image."],"metadata":{"id":"nqt3UjGA2zmS"},"id":"nqt3UjGA2zmS"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator_lpips.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"TZve2kdp_2lF"},"id":"TZve2kdp_2lF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Input image:')\n","plot_image(input_img[0])"],"metadata":{"id":"YZcaz0q63G9y"},"id":"YZcaz0q63G9y","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The test errors for this new model are:"],"metadata":{"id":"dv1RLKHfNMZp"},"id":"dv1RLKHfNMZp"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator_lpips, discriminator_lpips, generator_loss, discriminator_loss))"],"metadata":{"id":"ERo3uzBlNQNm"},"id":"ERo3uzBlNQNm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##8. Using WGAN (with gradient penalty) as training mechanism\n"],"metadata":{"id":"e6a3FqSiwTN2"},"id":"e6a3FqSiwTN2"},{"cell_type":"markdown","source":["The Wassertstein Generative Adversarial Network (WGAN) is a variant of GAN proposed in 2017. It is useful because produces higher model stability and the loss acquires meaning due to termination criteria. It also prevents model collapse."],"metadata":{"id":"yG6yd9RsSaAq"},"id":"yG6yd9RsSaAq"},{"cell_type":"markdown","source":["### Changes introduced"],"metadata":{"id":"RjFdHdGb3bph"},"id":"RjFdHdGb3bph"},{"cell_type":"markdown","source":["The WGAN architecture requires also implementing a gradient penalty function:"],"metadata":{"id":"m3kOSm7hNu0_"},"id":"m3kOSm7hNu0_"},{"cell_type":"code","source":["def compute_gp(discriminator, real_data, fake_data):\n","  \n","  batch_size = real_data.size(0)\n","  \n","  # Sample Epsilon from uniform distribution\n","  eps = torch.rand(batch_size, 1, 1, 1).to(real_data.device)\n","  eps = eps.expand_as(real_data)\n","        \n","  # Interpolation between real data and fake data.\n","  interpolation = eps * real_data + (1 - eps) * fake_data\n","        \n","  # get logits for interpolated images\n","  interp_logits = discriminator(interpolation)\n","  grad_outputs = torch.ones_like(interp_logits)\n","        \n","  # Compute Gradients\n","  gradients = autograd.grad(\n","    outputs=interp_logits,\n","    inputs=interpolation,\n","    grad_outputs=grad_outputs,\n","    create_graph=True,\n","    retain_graph=True,\n","  )[0]\n","        \n","  # Compute and return Gradient Norm\n","  gradients = gradients.view(batch_size, -1)\n","  grad_norm = gradients.norm(2, 1)\n","  return torch.mean((grad_norm - 1) ** 2)"],"metadata":{"id":"UenzlqaZzhsx","executionInfo":{"status":"ok","timestamp":1674722499863,"user_tz":-60,"elapsed":302,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"UenzlqaZzhsx","execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["And we will also use another discriminator loss function:"],"metadata":{"id":"5ZYsyWgUNz9l"},"id":"5ZYsyWgUNz9l"},{"cell_type":"code","source":["def discriminator_loss_wgan(output, label, gradient_penalty, c_lambda=10):\n","  return torch.mean(output - label  + gradient_penalty * c_lambda)"],"metadata":{"id":"7OityPTn7Aej","executionInfo":{"status":"ok","timestamp":1674722504326,"user_tz":-60,"elapsed":299,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"7OityPTn7Aej","execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"5m8H-QdO3hf0"},"id":"5m8H-QdO3hf0"},{"cell_type":"code","source":["CRITIC_ITERATIONS = 5\n","\n","generator_wgan = Generator(3)\n","generator_wgan.to(device)\n","generator_optimizer_wgan = op.Adam(generator_wgan.parameters(), lr=0.0002, weight_decay=0.5)\n","\n","critic_wgan = Discriminator(in_channels=6, kernel_size=4)\n","critic_wgan.to(device)\n","critic_optimizer_wgan = op.Adam(critic_wgan.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"w1yjYZ708EIC","executionInfo":{"status":"ok","timestamp":1674722537157,"user_tz":-60,"elapsed":2204,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"w1yjYZ708EIC","execution_count":21,"outputs":[]},{"cell_type":"code","source":["GENERATOR_WGAN_PATH = \"drive/MyDrive/models/generatorModelImageColoringWGAN.pth\"\n","DISCRIMINATOR_WGAN_PATH = \"drive/MyDrive/models/discriminatorModelImageColoringWGAN.pth\"\n","\n","generator_wgan_losses = list()\n","discriminator_wgan_losses = list()\n","\n","NUM_EPOCHS = 10\n","cnt = 1\n","\n","for epoch in range(1, NUM_EPOCHS + 1):\n","  for (input_img, tg_img) in dataloader:\n","\n","    critic_optimizer_wgan.zero_grad()\n","\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = generator_wgan(input_img)\n","\n","    # NEW: discriminator will be updated CRITIC_ITERATIONS each batch processed\n","    for _ in range(CRITIC_ITERATIONS):\n","\n","      disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","      discriminator_fake = critic_wgan(disc_inp_fake.detach())\n","      \n","      disc_inp_real = torch.cat((input_img, tg_img), 1)\n","      discriminator_real = critic_wgan(disc_inp_real)\n","\n","      # NEW: we are calculating the gradient penalty\n","      gradient_penalty = compute_gp(critic_wgan, disc_inp_real, disc_inp_fake)\n","\n","      # NEW: the discriminator loss has changed\n","      discriminator_fake_loss = discriminator_loss_wgan(discriminator_fake, fake_target, gradient_penalty)\n","      discriminator_real_loss = discriminator_loss_wgan(discriminator_real,  real_target, gradient_penalty)\n","      \n","      discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","      discriminator_wgan_losses.append(discriminator_total_loss.item())\n","\n","      discriminator_total_loss.backward(retain_graph=True)\n","      critic_optimizer_wgan.step()\n","\n","    generator_optimizer_wgan.zero_grad()\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = critic_wgan(fake_gen)\n","    \n","    gen_loss = generator_loss(generated_image, tg_img, discriminator_prediction, real_target)\n","\n","    generator_wgan_losses.append(gen_loss.item())\n","\n","    gen_loss.backward(retain_graph=True)\n","    generator_optimizer_wgan.step()\n","\n","    if cnt % 10 == 0:\n","      print(\"CURRENT STATE: Epoch: {0}, Number of batches processed: {1}\".format(epoch, cnt))\n","      save_model(generator_wgan, GENERATOR_WGAN_PATH)\n","      save_model(critic_wgan, DISCRIMINATOR_WGAN_PATH)\n","    \n","    cnt = cnt + 1"],"metadata":{"id":"Q4lpXy0M7_tQ"},"id":"Q4lpXy0M7_tQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plots for the generator and discriminator loss are the following:"],"metadata":{"id":"OGYYbSwjOns1"},"id":"OGYYbSwjOns1"},{"cell_type":"code","source":["plot_gen_loss(generator_wgan_losses)\n","plot_disc_loss(discriminator_wgan_losses)"],"metadata":{"id":"8Y5sykuSOtCZ"},"id":"8Y5sykuSOtCZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"13SuSAqT3xUU"},"id":"13SuSAqT3xUU"},{"cell_type":"markdown","source":["Now let's test the model we obtained showing the generator output for the training image."],"metadata":{"id":"W4uMshN-O-iu"},"id":"W4uMshN-O-iu"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator_wgan.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"tIjh_PSP30cm"},"id":"tIjh_PSP30cm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Input image:')\n","plot_image(input_img[0])"],"metadata":{"id":"K6KeyJa8Q4xc"},"execution_count":null,"outputs":[],"id":"K6KeyJa8Q4xc"},{"cell_type":"markdown","source":["The test errors for this new model are:"],"metadata":{"id":"4lvSsaxDPCmx"},"id":"4lvSsaxDPCmx"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator_wgan, critic_wgan, generator_loss, discriminator_loss))"],"metadata":{"id":"5pXnx4-MPDF-"},"id":"5pXnx4-MPDF-","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##9. Using WGAN and additional perceptual losses\n"],"metadata":{"id":"1BedRzhXRGvl"},"id":"1BedRzhXRGvl"},{"cell_type":"markdown","source":["###Training"],"metadata":{"id":"6SbnOUvKS0Hb"},"id":"6SbnOUvKS0Hb"},{"cell_type":"markdown","source":["Now let's test the model we obtained showing the generator output for the training image."],"metadata":{"id":"9yjqeH3p2f_7"},"id":"9yjqeH3p2f_7"},{"cell_type":"code","source":["CRITIC_ITERATIONS = 5\n","\n","generator_all = Generator(3)\n","generator_all.to(device)\n","generator_optimizer_all = op.Adam(generator_all.parameters(), lr=0.0002, weight_decay=0.5)\n","\n","critic_all = Discriminator(in_channels=6, kernel_size=4)\n","critic_all.to(device)\n","critic_optimizer_all = op.Adam(critic_all.parameters(), lr=0.0002, weight_decay=0.5)"],"metadata":{"id":"0UvQKz8IPa98","executionInfo":{"status":"ok","timestamp":1674722557739,"user_tz":-60,"elapsed":1163,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"id":"0UvQKz8IPa98","execution_count":24,"outputs":[]},{"cell_type":"code","source":["GENERATOR_ALL_PATH = \"drive/MyDrive/models/generatorModelImageColoringWGANandLpips.pth\"\n","DISCRIMINATOR_ALL_PATH = \"drive/MyDrive/models/discriminatorModelImageColoringWGANandLpips.pth\"\n","\n","generator_all_losses = list()\n","discriminator_all_losses = list()\n","\n","NUM_EPOCHS = 10\n","cnt = 1\n","\n","for epoch in range(1, NUM_EPOCHS + 1):\n","  for (input_img, tg_img) in dataloader:\n","\n","    critic_optimizer_all.zero_grad()\n","\n","    input_img = input_img.to(device)\n","    tg_img = tg_img.to(device)\n","\n","    real_target = torch.ones(input_img.size(0), 1, 30, 30).to(device)\n","    fake_target = torch.zeros(input_img.size(0), 1, 30, 30).to(device)\n","\n","    generated_image = generator_all(input_img)\n","\n","    for _ in range(CRITIC_ITERATIONS):\n","\n","      disc_inp_fake = torch.cat((input_img, generated_image), 1)\n","      discriminator_fake = critic_all(disc_inp_fake.detach())\n","      \n","      disc_inp_real = torch.cat((input_img, tg_img), 1)\n","      discriminator_real = critic_all(disc_inp_real)\n","\n","      gradient_penalty = compute_gp(critic_all, disc_inp_real, disc_inp_fake)\n","\n","      discriminator_fake_loss = discriminator_loss_wgan(discriminator_fake, fake_target, gradient_penalty)\n","      discriminator_real_loss = discriminator_loss_wgan(discriminator_real,  real_target, gradient_penalty)\n","\n","      discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","\n","      discriminator_all_losses.append(discriminator_total_loss.item())\n","\n","      discriminator_total_loss.backward(retain_graph=True)\n","      critic_optimizer_all.step()\n","\n","    generator_optimizer_all.zero_grad()\n","    fake_gen = torch.cat((input_img, generated_image), 1)\n","    discriminator_prediction = critic_all(fake_gen)\n","\n","    gen_loss = generator_loss_lpips(generated_image, tg_img, discriminator_prediction, real_target)                              \n","\n","    generator_all_losses.append(gen_loss.item())\n","\n","    gen_loss.backward(retain_graph=True)\n","    generator_optimizer_all.step()\n","\n","    if cnt % 10 == 0:\n","      print(\"CURRENT STATE: Epoch: {0}, Number of batches processed: {1}\".format(epoch, cnt))\n","      save_model(generator_all, GENERATOR_ALL_PATH)\n","      save_model(critic_all, DISCRIMINATOR_ALL_PATH)\n","    \n","    cnt = cnt + 1"],"metadata":{"id":"pPuE_7xHRTLM"},"id":"pPuE_7xHRTLM","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plots for the generator and discriminator loss are the following:"],"metadata":{"id":"ZMmnS2bcRk9U"},"id":"ZMmnS2bcRk9U"},{"cell_type":"code","source":["plot_gen_loss(generator_all_losses)\n","plot_disc_loss(discriminator_all_losses)"],"metadata":{"id":"LKZPcISMRk9V","executionInfo":{"status":"aborted","timestamp":1674678222559,"user_tz":-60,"elapsed":7,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"execution_count":null,"outputs":[],"id":"LKZPcISMRk9V"},{"cell_type":"markdown","source":["###Test"],"metadata":{"id":"6-gF7PVqStxI"},"id":"6-gF7PVqStxI"},{"cell_type":"code","source":["# TEST\n","input_img, tg_img = dataset[0]\n","input_img = input_img.unsqueeze(0)\n","tg_img = tg_img.unsqueeze(0)\n","\n","out = generator_all.forward(input_img)\n","\n","out = out.detach()\n","\n","print('Generated image:')\n","plot_image_minus1to1(out[0])"],"metadata":{"id":"eRj1T_qnSUFE","executionInfo":{"status":"aborted","timestamp":1674678222560,"user_tz":-60,"elapsed":8,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"execution_count":null,"outputs":[],"id":"eRj1T_qnSUFE"},{"cell_type":"markdown","source":["The test errors for this new model are:"],"metadata":{"id":"M_WBZIANRGUt"},"id":"M_WBZIANRGUt"},{"cell_type":"code","source":["print(get_test_error(dataloader_test, generator_all, critic_all, generator_loss, discriminator_loss))"],"metadata":{"id":"POGznJprRGUv","executionInfo":{"status":"aborted","timestamp":1674678222561,"user_tz":-60,"elapsed":8,"user":{"displayName":"Jesús Moncada Ramírez","userId":"03809042163319486653"}}},"execution_count":null,"outputs":[],"id":"POGznJprRGUv"},{"cell_type":"markdown","source":["## 10. Final comparison among models"],"metadata":{"id":"AmzDZ45G0-3r"},"id":"AmzDZ45G0-3r"},{"cell_type":"markdown","source":["First, we will load the four models obtained. Note that the final trained models must be in the folder <code>drive/MyDrive/models/FinalModels/</code>."],"metadata":{"id":"6athBH6V1MyI"},"id":"6athBH6V1MyI"},{"cell_type":"code","source":["gen_bas = Generator(3)\n","gen_bas = load_model(gen_bas, 'drive/MyDrive/models/FinalModels/BASIC/generatorModelImageColoring.pth', verbose=True)\n","dis_bas = Discriminator(in_channels=6, kernel_size=4)\n","dis_bas = load_model(dis_bas, 'drive/MyDrive/models/FinalModels/BASIC/discriminatorModelImageColoring.pth', verbose=True)\n","\n","gen_lpips = Generator(3)\n","gen_lpips = load_model(gen_lpips, 'drive/MyDrive/models/FinalModels/LPIPS/generatorModelImageColoringLpips.pth', verbose=True)\n","dis_lpips = Discriminator(in_channels=6, kernel_size=4)\n","dis_lpips = load_model(dis_lpips, 'drive/MyDrive/models/FinalModels/LPIPS/discriminatorModelImageColoringLpips.pth', verbose=True)\n","\n","gen_wgan = Generator(3)\n","gen_wgan = load_model(gen_wgan, 'drive/MyDrive/models/FinalModels/WGAN/generatorModelImageColoringWGAN.pth', verbose=True)\n","dis_wgan = Discriminator(in_channels=6, kernel_size=4)\n","dis_wgan = load_model(dis_wgan, 'drive/MyDrive/models/FinalModels/WGAN/discriminatorModelImageColoringWGAN.pth', verbose=True)\n","\n","gen_all = Generator(3)\n","gen_all = load_model(gen_all, 'drive/MyDrive/models/FinalModels/WGAN&LPIPS/generatorModelImageColoringWGANandLpips.pth', verbose=True)\n","dis_all = Discriminator(in_channels=6, kernel_size=4)\n","dis_all = load_model(dis_all, 'drive/MyDrive/models/FinalModels/WGAN&LPIPS/discriminatorModelImageColoringWGANandLpips.pth', verbose=True)"],"metadata":{"id":"gic7EfPT1ShL"},"id":"gic7EfPT1ShL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's plot the generator output for the same input image. We won't take the input image from the training set."],"metadata":{"id":"8PBKAlZ54W-a"},"id":"8PBKAlZ54W-a"},{"cell_type":"code","source":["input_img, target_img = dataset[9000]\n","input_img = input_img.unsqueeze(0)\n","target_img = target_img.unsqueeze(0)\n","\n","print('Plotting input image:')\n","plot_image(input_img[0])\n","\n","out_bas = gen_bas(input_img).detach()\n","out_lpips = gen_lpips(input_img).detach()\n","out_wgan = gen_wgan (input_img).detach()\n","out_all = gen_all(input_img).detach()\n","\n","print('Plotting results:')\n","plot_4_images_minus1to1(out_bas[0], out_lpips[0], out_wgan[0], out_all[0])"],"metadata":{"id":"oBrkH9eo4jaz"},"id":"oBrkH9eo4jaz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will repeat the process for another image."],"metadata":{"id":"IDRUTiP2_SGE"},"id":"IDRUTiP2_SGE"},{"cell_type":"code","source":["input_img, target_img = dataset[9200]\n","input_img = input_img.unsqueeze(0)\n","target_img = target_img.unsqueeze(0)\n","\n","print('Plotting input image:')\n","plot_image(input_img[0])\n","\n","out_bas = gen_bas(input_img).detach()\n","out_lpips = gen_lpips(input_img).detach()\n","out_wgan = gen_wgan (input_img).detach()\n","out_all = gen_all(input_img).detach()\n","\n","print('Plotting results:')\n","plot_4_images_minus1to1(out_bas[0], out_lpips[0], out_wgan[0], out_all[0])"],"metadata":{"id":"wHXv6NEZ_WeN"},"id":"wHXv6NEZ_WeN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's calculate both generator test error and discriminator test error for every model."],"metadata":{"id":"V_xa31C-AiR2"},"id":"V_xa31C-AiR2"},{"cell_type":"code","source":["print('TEST ERROR BASIC', get_test_error(dataloader_test, gen_bas, dis_bas, generator_loss, discriminator_loss))\n","print('TEST ERROR LPIPS', get_test_error(dataloader_test, gen_lpips, dis_lpips, generator_loss, discriminator_loss))\n","print('TEST ERROR WGAN', get_test_error(dataloader_test, gen_wgan, dis_wgan, generator_loss, discriminator_loss))\n","print('TEST ERROR ALL', get_test_error(dataloader_test, gen_all, dis_all, generator_loss, discriminator_loss))"],"metadata":{"id":"cVVumOYEATof"},"id":"cVVumOYEATof","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}