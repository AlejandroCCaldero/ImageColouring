{"cells":[{"cell_type":"markdown","source":["<h1>Image Recolouring Project</h1>\n","\n","Project developed by Alejandro Cano Caldero and Jesús Moncada Ramírez for the subject Neural Networks and Deep Learning, University of Padova, 2022-23.\n"],"metadata":{"id":"XRlCDiXymmvi"},"id":"XRlCDiXymmvi"},{"cell_type":"code","execution_count":1,"id":"33804e43","metadata":{"id":"33804e43","executionInfo":{"status":"ok","timestamp":1673974716705,"user_tz":-60,"elapsed":3640,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"outputs":[],"source":["from PIL import Image\n","\n","import numpy as np\n","\n","import torch\n","\n","import matplotlib.pyplot as plt\n","\n","from torchvision import transforms, datasets\n","from torchvision.transforms import transforms\n","\n","from torch.utils.data import DataLoader, Dataset  \n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as op"]},{"cell_type":"code","execution_count":2,"id":"b6b86a5a","metadata":{"id":"b6b86a5a","executionInfo":{"status":"ok","timestamp":1673974720687,"user_tz":-60,"elapsed":244,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"outputs":[],"source":["# Define the execution device \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OyX2-8kfpi1z","outputId":"49fa45d6-5fb7-4d1a-e3b7-9cd14bfd8a9a","executionInfo":{"status":"ok","timestamp":1673974748301,"user_tz":-60,"elapsed":2831,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"OyX2-8kfpi1z","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["<h2>1. Dataset</h2>\n","\n","For the dataset we have used [ImageNette](https://github.com/fastai/imagenette), a reduced version of ImageNet, specifically the fill size images version."],"metadata":{"id":"yo4NF3Wam8QA"},"id":"yo4NF3Wam8QA"},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, image_path, transform=None):\n","        super(ImageDataset, self).__init__()\n","        self.data = datasets.ImageFolder(image_path,  transform)\n","\n","    def __getitem__(self, idx):\n","        x, y = self.data[idx]\n","        return x\n","\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"aGiWrxuRooTz","executionInfo":{"status":"ok","timestamp":1673974750644,"user_tz":-60,"elapsed":287,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"aGiWrxuRooTz","execution_count":5,"outputs":[]},{"cell_type":"code","source":["class TwoImagesDataset(Dataset):\n","  def __init__(self, dataset1, dataset2):\n","        super(TwoImagesDataset, self).__init__()\n","        self.dataset1 = dataset1\n","        self.dataset2 = dataset2\n","\n","  def __getitem__(self, idx):\n","        return self.dataset1[idx], self.dataset2[idx]\n","\n","  def __len__(self):\n","        return len(self.dataset1)"],"metadata":{"id":"UEXGx53VUtuq","executionInfo":{"status":"ok","timestamp":1673974759586,"user_tz":-60,"elapsed":273,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"UEXGx53VUtuq","execution_count":6,"outputs":[]},{"cell_type":"code","source":["img_path = 'drive/MyDrive/imagenette2/train'\n","\n","# Define normalization [0, 255] --> [-1, 1] (Owing to the use of the tanh activation function)\n","\n","colored_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","grayscale_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Grayscale(3),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","colored_data = ImageDataset(img_path, colored_transform)\n","grayscale_data = ImageDataset(img_path, grayscale_transform)\n","dataset = TwoImagesDataset(colored_data, grayscale_data)"],"metadata":{"id":"yIwpJ3fipDXP","executionInfo":{"status":"ok","timestamp":1673974774044,"user_tz":-60,"elapsed":12018,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"yIwpJ3fipDXP","execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"id":"83b897bd","metadata":{"id":"83b897bd","executionInfo":{"status":"ok","timestamp":1673974777852,"user_tz":-60,"elapsed":4,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"outputs":[],"source":["def print_image(img):\n","  plt.imshow(img.permute(1, 2, 0))"]},{"cell_type":"code","execution_count":9,"id":"e32d0b1e","metadata":{"id":"e32d0b1e","executionInfo":{"status":"ok","timestamp":1673974780734,"user_tz":-60,"elapsed":391,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels, kernel_size, stride=1):\n","        super().__init__()\n","        \n","        self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer2_bn = nn.BatchNorm2d(128)\n","        self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=kernel_size, stride=2, padding=1)\n","        self.layer3_bn = nn.BatchNorm2d(256)\n","        self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=kernel_size, padding=1) # stride = 1\n","        self.layer4_bn = nn.BatchNorm2d(512)\n","        self.layer5 = nn.Conv2d(512, out_channels=1, kernel_size=kernel_size, padding=1)\n","        \n","    \n","    def forward(self, x):\n","        d = F.leaky_relu(self.layer1(x), 0.2)\n","        print(d.shape)\n","        d = F.leaky_relu(self.layer2_bn(self.layer2(d)), 0.2)\n","        print(d.shape)\n","        d = F.leaky_relu(self.layer3_bn(self.layer3(d)), 0.2)\n","        print(d.shape)\n","        d = F.leaky_relu(self.layer4_bn(self.layer4(d)), 0.2)\n","        print(d.shape)\n","        d = self.layer5(d)\n","        print(d.shape)\n","        \n","        # Each (1×1) of the 30×30 represents a 70×70 dimension \n","        # in the input image (256×256), classifying a single patch of the original \n","        # image as real or fake.\n","        return torch.sigmoid(d)\n","        \n","\n","discriminator = Discriminator(3, 4)\n","discriminator.to(device)\n","\n","loss_fn = nn.BCELoss(weight=torch.tensor(0.5))\n","optimizer = op.Adam(discriminator.parameters(), lr=0.0002, weight_decay=0.5)"]},{"cell_type":"code","source":["for input_img, tg_img in dataset:\n","  disc_in = torch.cat((input_img, tg_img), 1)\n","  var = discriminator.forward(disc_in.unsqueeze(0))\n","  print(var)\n","  # print(var.shape)"],"metadata":{"id":"VQUWlIcaQC5W"},"id":"VQUWlIcaQC5W","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Generator(nn.Module):\n","  def __init__(self, in_channels, stride=1):\n","    super().__init__()\n","\n","    # ----------------------------- ENCODER ----------------------------\n","    #            ENCODER MODEL: C64-C128-C256-C512-C512-C512-C512\n","    self.layer1 = nn.Conv2d(in_channels, out_channels=64, kernel_size=4, stride=2, padding=1)\n","    self.layer2 = nn.Conv2d(64, out_channels=128, kernel_size=4, stride=2, padding=1)\n","    self.layer2_bn = nn.BatchNorm2d(128)\n","    self.layer3 = nn.Conv2d(128, out_channels=256, kernel_size=4, stride=2, padding=1)\n","    self.layer3_bn = nn.BatchNorm2d(256)\n","    self.layer4 = nn.Conv2d(256, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer4_bn = nn.BatchNorm2d(512)\n","    self.layer5 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer5_bn = nn.BatchNorm2d(512)\n","    self.layer6 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer6_bn = nn.BatchNorm2d(512)\n","    self.layer7 = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer7_bn = nn.BatchNorm2d(512)\n","    # ----------------------------- /ENCODER ----------------------------\n","\n","\n","    # ----------------------------- BOTTLENECK ----------------------------\n","    self.bottleneck_layer = nn.Conv2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    # ----------------------------- /BOTTLENECK ----------------------------\n","\n","\n","    # ----------------------------- DECODER ----------------------------\n","    #           DECODER MODEL: CD512-CD512-CD512-CD512-CD256-CD128-CD64\n","    self.layer8 = nn.ConvTranspose2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer8_bn = nn.BatchNorm2d(512)\n","    self.layer8_dpout = nn.Dropout()\n","    self.layer9 = nn.ConvTranspose2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer9_bn = nn.BatchNorm2d(512)\n","    self.layer9_dpout = nn.Dropout()\n","    self.layer10 = nn.ConvTranspose2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer10_bn = nn.BatchNorm2d(512)\n","    self.layer10_dpout = nn.Dropout()\n","    self.layer11 = nn.ConvTranspose2d(512, out_channels=512, kernel_size=4, stride=2, padding=1)\n","    self.layer11_bn = nn.BatchNorm2d(512)\n","    self.layer12 = nn.ConvTranspose2d(512, out_channels=256, kernel_size=4, stride=2, padding=1)\n","    self.layer12_bn = nn.BatchNorm2d(256)\n","    self.layer13 = nn.ConvTranspose2d(256, out_channels=128, kernel_size=4, stride=2, padding=1)\n","    self.layer13_bn = nn.BatchNorm2d(128)\n","    self.layer14 = nn.ConvTranspose2d(128, out_channels=64, kernel_size=4, stride=2, padding=1)\n","    self.layer14_bn = nn.BatchNorm2d(64)\n","    # ----------------------------- /DECODER ----------------------------\n","\n","    # ----------------------------- OUTPUT ----------------------------\n","    self.layer15 = nn.ConvTranspose2d(64, out_channels=3, kernel_size=4, stride=2, padding=1)\n","\n","  def forward(self, x):\n","\n","    # ----------------------------- ENCODER ----------------------------\n","    e1 = F.leaky_relu(self.layer1(x), 0.2)\n","    e2 = F.leaky_relu(self.layer2_bn(self.layer2(e1)), 0.2)\n","    e3 = F.leaky_relu(self.layer3_bn(self.layer3(e2)), 0.2)\n","    e4 = F.leaky_relu(self.layer4_bn(self.layer4(e3)), 0.2)\n","    e5 = F.leaky_relu(self.layer5_bn(self.layer5(e4)), 0.2)\n","    e6 = F.leaky_relu(self.layer6_bn(self.layer6(e5)), 0.2)\n","    e7 = F.leaky_relu(self.layer7_bn(self.layer7(e6)), 0.2) \n","    # ----------------------------- /ENCODER ----------------------------\n","\n","\n","    # ----------------------------- BOTTLENECK ----------------------------\n","    b = F.relu(self.bottleneck_layer(e7))\n","\n","\n","    # ----------------------------- DECODER ----------------------------\n","    d1 = F.relu(torch.cat((self.layer8_dpout(self.layer8_bn(self.layer8(b))), e7)))\n","    d2 = F.relu(torch.cat((self.layer9_dpout(self.layer9_bn(self.layer9(d1))), e6)))\n","    d3 = F.relu(torch.cat((self.layer10_dpout(self.layer10_bn(self.layer10(d2))), e5)))\n","    d4 = F.relu(torch.cat((self.layer11_bn(self.layer11(d3))), e4))\n","    d5 = F.relu(torch.cat((self.layer12_bn(self.layer12(d4))), e3))\n","    d6 = F.relu(torch.cat((self.layer13_bn(self.layer13(d5))), e2))\n","    d7 = F.relu(torch.cat((self.layer14_bn(self.layer14(d6))), e1))\n","\n","    # ----------------------------- OUTPUT ----------------------------\n","    o = F.tanh(self.layer15(d7))\n","\n","    return o"],"metadata":{"id":"g6SCuigGi7WC","executionInfo":{"status":"ok","timestamp":1673976205136,"user_tz":-60,"elapsed":249,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"g6SCuigGi7WC","execution_count":15,"outputs":[]},{"cell_type":"code","source":["generator = Generator(3)"],"metadata":{"id":"w90yleohsi_C","executionInfo":{"status":"ok","timestamp":1673976208584,"user_tz":-60,"elapsed":266,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}}},"id":"w90yleohsi_C","execution_count":16,"outputs":[]},{"cell_type":"code","source":["for input_img, tg_img in dataset:\n","  var = generator.forward(input_img.unsqueeze(0))\n","  print(var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"id":"OdfwyxIqszxO","executionInfo":{"status":"error","timestamp":1673976270398,"user_tz":-60,"elapsed":2977,"user":{"displayName":"Alejandro Cano","userId":"13380801265196036937"}},"outputId":"f0ced2c5-7ba1-4645-c0f3-179c1d24c746"},"id":"OdfwyxIqszxO","execution_count":19,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-158c36a321ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtg_img\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-87f012f15522>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# ----------------------------- DECODER ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer8_dpout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer8_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer9_dpout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer9_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0md3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer10_dpout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer10_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 2 but got size 3 for tensor number 1 in the list."]}]},{"cell_type":"code","source":["adversarial_loss = nn.BCELoss(weight=torch.tensor(0.5))\n","l1_loss = nn.L1Loss()"],"metadata":{"id":"e45LZ-VXN-R5"},"id":"e45LZ-VXN-R5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generator_loss(generator_image, target_image, discriminator_predictions, real_target):\n","  gen_loss = adversarial_loss(discriminator_predictions, real_target)\n","  l1_l = l1_loss(generator_image, target_image)\n","  result = gen_loss + (100 * l1_l)\n","\n","  return result"],"metadata":{"id":"FhDadAG5OKD1"},"id":"FhDadAG5OKD1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def discriminator_loss(output, label):\n","  return adversarial_loss(output, label)"],"metadata":{"id":"lbRSIgnfOPDV"},"id":"lbRSIgnfOPDV","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}